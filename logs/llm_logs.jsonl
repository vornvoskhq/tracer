=== LLM RUN ===
time=2025-11-29 13:52 | model=openai/gpt-4o-mini | preset=entrypoints | kind=entrypoints | codebase=vgmini | temp=0.10 | max_tok=512 | in=4855 | out=458 | dur=10.632s | cost=0.001003
command=./vg pipeline --model equal_weight_dual

PROMPT (instructions only):
You are helping a developer understand a new, black-box Python codebase. You will be given snippets from many .py files in this project.

Your tasks:
- Identify which file(s) are most likely to act as entrypoints (top-level scripts or main modules).
- For each candidate entrypoint, briefly explain why it is likely an entrypoint (e.g., has an if __name__ == '__main__' block, defines a main() that parses CLI args, or is clearly the starting script).
- If there appears to be a thin wrapper script that quickly hands off control to a deeper module or framework, explain that layering and suggest which deeper file is the 'real' place to start reading.
- Provide a short recommendation section: 'If you want to understand this application, start by reading: ...'.

Project file snippets:

FILES SENT:
  - __init__.py
  - vgmini.py
  - debug_cli.py
  - infer_cli.py
  - tests/conftest.py
  - tests/test_yaml_merge.py
  - tests/test_cluster_resolution.py
  - tests/test_target_strategy.py
  - tests/test_model_robustness.py
  - scripts/__init__.py
  - scripts/tune_equal_weight.py
  - src/__init__.py

RESPONSE:
### Identifying Entrypoints in the Codebase

Based on the provided snippets, the following files are likely candidates for entrypoints in the `vgmini` project:

1. **vgmini.py**
   - **Reason**: This file contains a shebang (`#!/usr/bin/env python3`) and a docstring indicating it serves as a "Single Entry Point" for the VGMini Trading Framework. It likely contains the main logic for running the application, as it imports various modules and defines functions that are essential for the framework's operation.

2. **debug_cli.py**
   - **Reason**: This file also has a shebang and includes an `if __name__ == "__main__":` block, which indicates that it can be executed as a standalone script. It serves as a wrapper that filters command-line arguments before passing them to `infer_cli.py`, suggesting it is designed for debugging purposes.

3. **infer_cli.py**
   - **Reason**: This file has a shebang and a detailed docstring explaining its purpose as a serverless CLI for on-demand inference. It also includes argument parsing and is structured to read input from stdin, making it a likely entry point for inference tasks within the framework.

4. **scripts/tune_equal_weight.py**
   - **Reason**: This file has a shebang and a docstring that describes its functionality as a tuner for parameters. It is designed to be run as a script, indicating it serves a specific purpose within the application.

### Layering and Deeper Modules

- **debug_cli.py** acts as a thin wrapper around **infer_cli.py**. It filters arguments before passing them along, which means that while `debug_cli.py` can be executed directly, the core functionality resides in `infer_cli.py`. If a developer wants to understand the inference logic, they should focus on `infer_cli.py`.

### Recommendations

If you want to understand this application, start by reading:
- **vgmini.py** for an overview of the framework's entry point and main functionalities.
- **infer_cli.py** for detailed insights into the inference process and how it interacts with the data.
- **debug_cli.py** if you are interested in debugging capabilities and argument handling for the CLI.

=== LLM RUN ===
time=2025-11-29 14:11 | model=moonshotai/kimi-k2-thinking | preset=refactor-ai-mess | kind=function | codebase=vgmini | temp=0.10 | max_tok=512 | in=16663 | out=2587 | dur=52.368s | cost=NA
command=./vg pipeline --model equal_weight_dual

PROMPT:
You are an expert Python engineer. Review the following function or execution slice, which was likely generated or heavily edited by AI. Provide a concise analysis that covers:
- Where the structure is overcomplicated, redundant, or over-abstracted
- Any dead code, unused branches, or unnecessary configuration flags
- Any use of synthetic or fallback data paths that should be removed or made explicit (for technical analysis software, there must be no hidden fake data).

Then propose a refactor plan:
- How to simplify while preserving behavior and data integrity
- Which pieces can safely be deleted vs. just reorganized
- A short checklist of steps to get from the current version to a cleaner one

Function source or trace context:
```python
#!/usr/bin/env python3
"""
VGMini Trading Framework - Single Entry Point
Professional command-line interface for running trading experiments
"""

import argparse
import json
import os
import sys
import subprocess
import shutil
from datetime import datetime, timedelta
from typing import List, Dict, Any
from pathlib import Path

def check_virtual_environment():
    """Check if we're running in a virtual environment"""
    return hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)

def get_venv_python():
    """Get the path to the virtual environment Python executable"""
    if os.name == 'nt':  # Windows
        return os.path.join('.venv', 'Scripts', 'python.exe')
    else:  # Unix/Linux/macOS
        return os.path.join('.venv', 'bin', 'python')

def setup_virtual_environment():
    """Create and setup virtual environment with comprehensive error handling"""
    print("ğŸ”§ Setting up virtual environment...")
    
    # Create venv if it doesn't exist
    if not os.path.exists('.venv'):
        print("ğŸ“¦ Creating virtual environment...")
        try:
            subprocess.run([sys.executable, '-m', 'venv', '.venv'], 
                         check=True, stderr=subprocess.PIPE, text=True)
            print("âœ… Virtual environment created")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to create virtual environment: {e.stderr}")
            print("ğŸ’¡ Try installing python3-venv: sudo apt-get install python3-venv")
            sys.exit(1)
    
    venv_python = get_venv_python()
    
    # Ensure pip is up to date first
    try:
        print("ğŸ”„ Upgrading pip and setuptools...")
        subprocess.run(
            [venv_python, '-m', 'pip', 'install', '--upgrade', 'pip', 'setuptools'],
            check=True, stderr=subprocess.PIPE, text=True, stdout=subprocess.DEVNULL
        )
    except subprocess.CalledProcessError as e:
        print(f"âš ï¸  Failed to upgrade pip: {e.stderr}")
    
    # Install dependencies from pyproject.toml if it exists
    if os.path.exists('pyproject.toml'):
        print("ğŸ“¦ Installing project dependencies...")
        try:
            result = subprocess.run(
                [venv_python, '-m', 'pip', 'install', '-e', '.'],
                check=True, capture_output=True, text=True
            )
            print("âœ… Dependencies installed successfully")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to install dependencies. Error:\n{e.stderr}")
            print("\nğŸ’¡ Attempting individual package installation...")
            
            # Fallback to installing core dependencies individually
            core_deps = [
                'pandas>=2.0.0', 'numpy>=1.24.0', 'scikit-learn>=1.3.0',
                'matplotlib>=3.7.0', 'seaborn>=0.12.0', 'ta>=0.10.0'
            ]
            try:
                print("ğŸ“¦ Installing core dependencies...")
                subprocess.run(
                    [venv_python, '-m', 'pip', 'install'] + core_deps,
                    check=True, stderr=subprocess.PIPE, text=True
                )
                print("âœ… Core dependencies installed")
                
                # Try installing the package again
                print("ğŸ“¦ Installing package in development mode...")
                subprocess.run(
                    [venv_python, '-m', 'pip', 'install', '-e', '.'],
                    check=True, stderr=subprocess.PIPE, text=True
                )
                print("âœ… Package installed in development mode")
                
            except subprocess.CalledProcessError as e:
                print(f"âŒ Failed to install core dependencies: {e.stderr}")
                print("\nğŸ’¡ Please try installing the dependencies manually:")
                print("1. source .venv/bin/activate")
                print("2. pip install -e .")
                sys.exit(1)
    
    elif os.path.exists('requirements.txt'):
        print("ğŸ“¦ Installing dependencies from requirements.txt...")
        try:
            subprocess.run(
                [venv_python, '-m', 'pip', 'install', '-r', 'requirements.txt'],
                check=True, stderr=subprocess.PIPE, text=True
            )
            print("âœ… Dependencies installed from requirements.txt")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to install dependencies: {e.stderr}")
            sys.exit(1)
    else:
        print("âš ï¸  No pyproject.toml or requirements.txt found")
    
    return venv_python

def restart_with_venv():
    """Restart the script with the virtual environment Python"""
    if check_virtual_environment():
        # Already in venv, continue
        return False
    
    # Check if venv exists, create if not
    if not os.path.exists('.venv'):
        setup_virtual_environment()
    
    venv_python = get_venv_python()
    
    # Check if venv python exists
    if not os.path.exists(venv_python):
        setup_virtual_environment()
    
    # Restart with venv python, preserving module invocation if used
    print("ğŸ”„ Restarting with virtual environment...")
    argv0 = sys.argv[0]
    # If argv0 looks like a script path (ends with .py or exists on disk), run it as a script
    # Otherwise, assume it was invoked as a module name and preserve -m semantics
    if (argv0.endswith('.py') or os.path.exists(argv0)):
        new_argv = [venv_python, argv0] + sys.argv[1:]
    else:
        new_argv = [venv_python, '-m', argv0] + sys.argv[1:]
    os.execv(venv_python, new_argv)

# Validate we're in the right directory
if not os.path.exists('src') or not os.path.exists('vgmini.py'):
    print("âŒ Please run vgmini.py from the project root directory")
    print("ğŸ’¡ Make sure you're in the directory containing src/ and vgmini.py")
    sys.exit(1)

# Check and restart with venv if needed
restart_with_venv()

# Add src to path
sys.path.append('src')

try:
    import pandas as pd
    import matplotlib.pyplot as plt
    import numpy as np
    from src.main_framework import TradingFramework
    from src.experiment_configs import ExperimentConfig
except ImportError as e:
    print(f"âŒ Failed to import required modules: {e}")
    print("ğŸ”§ Try running: pip install -r requirements.txt")
    sys.exit(1)

def extract_buy_signals(ranking_file: str) -> str:
    """Extract only BUY signals from ranking file"""
    buy_signals_file = ranking_file.replace('.txt', '_buy_signals.txt')
    
    try:
        with open(ranking_file, 'r') as f:
            lines = f.readlines()
        
        print(f"DEBUG: Ranking file has {len(lines)} total lines")
        
        buy_lines = []
        buy_count = 0
        header_lines = 0
        
        # Debug: Show first few lines to understand format
        print("DEBUG: First 10 lines of ranking file:")
        for i, line in enumerate(lines[:10]):
            print(f"  Line {i+1}: {repr(line.strip())}")
        
        # Process each line
        for line_num, line in enumerate(lines):
            if line.startswith('#') or line.startswith('Rank') or line.startswith('-'):
                # Keep header lines
                buy_lines.append(line)
                header_lines += 1
                continue
                
            # Parse ranking line to check signal
            parts = line.strip().split()
            
            if len(parts) >= 6:  # Should have Rank, Symbol, Score1, Score2, Emoji, Signal columns
                signal = parts[5]  # Signal column (after emoji)
                if signal == 'BUY':
                    buy_lines.append(line)
                    buy_count += 1
        
        print(f"âœ“ Extracted {buy_count} BUY signals from {line_num+1} lines")
        
        # Write BUY signals file
        with open(buy_signals_file, 'w') as f:
            f.writelines(buy_lines)
        
        # Verify written file
        with open(buy_signals_file, 'r') as f:
            written_lines = f.readlines()
        print(f"DEBUG: Written file has {len(written_lines)} lines")
        
        print(f"Extracted {buy_count} BUY signals from {ranking_file}")
        return buy_signals_file
        
    except Exception as e:
        print(f"ERROR: Could not extract BUY signals: {e}")
        import traceback
        traceback.print_exc()
        return ranking_file


def count_lines(file_path: str) -> int:
    """Count non-header lines in file"""
    try:
        with open(file_path, 'r') as f:
            lines = f.readlines()
        return len([line for line in lines if not line.startswith('#') and not line.startswith('Rank') and not line.startswith('-') and line.strip()])
    except:
        return 0


def apply_trend_prefilter(ranking_file: str, limiter_json_file: str) -> str:
    """Pre-filter ranking file to remove bearish/consolidating stocks before decorrelation"""
    import json
    import pandas as pd
    from datetime import datetime
    import traceback
    
    print(f"ğŸ” Applying trend-based pre-filtering...")
    
    # Load limiter analysis results
    if not os.path.exists(limiter_json_file):
        print(f"ERROR: Limiter file not found: {limiter_json_file}")
        return ranking_file
    
    try:
        with open(limiter_json_file, 'r') as f:
            limiter_data = json.load(f)
        
        print(f"DEBUG: Loaded limiter data with keys: {list(limiter_data.keys())}")
        
        # Create lookup of trend classifications
        trend_lookup = {}
        if 'results' in limiter_data:
            print(f"DEBUG: Limiter has {len(limiter_data['results'])} results")
            for i, result in enumerate(limiter_data['results']):
                symbol = result.get('symbol')
                trend = result.get('trend')
                trend_strength = result.get('trend_strength', 0)
                trend_lookup[symbol] = (trend, trend_strength)
                if i < 5:  # Show first 5 for debugging
                    print(f"DEBUG: Trend lookup - {symbol}: {trend} ({trend_strength})")
        else:
            print("DEBUG: No 'results' key found in limiter data")
        
        print(f"DEBUG: Created trend lookup for {len(trend_lookup)} symbols")
        
        # Read original ranking file
        filtered_lines = []
        removed_stocks = []
        kept_stocks = []
        
        with open(ranking_file, 'r') as f:
            lines = f.readlines()
        
        print(f"ğŸ“Š Processing {len(lines)} ranking lines for trend filtering...")
        
        # Check for duplicate lines in input (common cause of duplicate processing)
        unique_lines = list(dict.fromkeys(lines))  # Remove duplicates while preserving order
        if len(unique_lines) != len(lines):
            print(f"âš ï¸  Removed {len(lines) - len(unique_lines)} duplicate lines")
            lines = unique_lines
        
        # Process each line with progress tracking
        processed_symbols = set()  # Track processed symbols to avoid double-processing
        total_lines = len(lines)
        for line_num, line in enumerate(lines):
            # Hypermodern gradient progress bar every 50 lines
            if line_num % 50 == 0 or line_num == total_lines - 1:
                progress = int(30 * (line_num + 1) / total_lines)
                bar = 'â–ˆ' * progress + 'â–‘' * (30 - progress)
                print(f"\r[{bar}] {line_num+1}/{total_lines} lines processed", end='', flush=True)
            if line.startswith('#') or line.startswith('Rank') or line.startswith('-'):
                # Keep header lines
                filtered_lines.append(line)
                continue
                
            # Parse ranking line
            parts = line.strip().split()
            if len(parts) >= 2:
                symbol = parts[1]
                
                # Skip if we've already processed this symbol (prevent duplicates)
                if symbol in processed_symbols:
                    continue
                processed_symbols.add(symbol)
                
                # Check trend classification
                if symbol in trend_lookup:
                    trend, trend_strength = trend_lookup[symbol]
                    
                    # Filter criteria: Only keep bullish and strong breakout trends
                    if trend == 'bullish' or (trend == 'breakout' and trend_strength > 0.7):
                        filtered_lines.append(line)
                        kept_stocks.append(f"{symbol}({trend})")
                    else:
                        removed_stocks.append(f"{symbol}({trend})")
                else:
                    # Keep if no trend data available
                    filtered_lines.append(line)
                    kept_stocks.append(f"{symbol}(no_data)")
            else:
                filtered_lines.append(line)
        
        # Complete progress bar
        print()  # New line after progress bar
        
        # Write filtered ranking file
        output_file = ranking_file.replace('.txt', '_trend_filtered.txt')
        with open(output_file, 'w') as f:
            f.writelines(filtered_lines)
        
        # Clean summary without excessive debug
        breakout_count = sum(1 for stock in kept_stocks if '(breakout)' in stock)
        bullish_count = sum(1 for stock in kept_stocks if '(bullish)' in stock)
        bearish_removed = sum(1 for stock in removed_stocks if '(bearish)' in stock)
        consolidating_removed = sum(1 for stock in removed_stocks if '(consolidating)' in stock)
        
        print(f"âœ“ Trend filtering complete: kept {len(kept_stocks)} stocks ({breakout_count} breakouts, {bullish_count} bullish)")
        print(f"  Removed {len(removed_stocks)} stocks ({bearish_removed} bearish, {consolidating_removed} consolidating)")
        
        
        return output_file
        
    except Exception as e:
        print(f"ERROR: Could not apply trend pre-filter: {e}")
        import traceback
        traceback.print_exc()
        return ranking_file


def run_pipeline(model_name: str, top_count: int = 100):
    """Run the complete VGMini pipeline: ranking -> decorrelation -> trailstop -> limiter -> charts"""
    import os
    from pathlib import Path
    
    # Running pipeline for model (reduce console clutter)
    
    # Step 1: Generate ranking
    print(f"[pipeline] Ranking top {top_count} symbols...")
    result = subprocess.run([sys.executable, __file__, 'rank', model_name, '--top', str(top_count), '--quiet'], 
                          capture_output=False)
    if result.returncode != 0:
        print(f"[pipeline] Ranking failed with return code {result.returncode}")
        return
    
    # Verify ranking file was created
    ranking_file = f"results/{model_name}_ranking.txt"
    if not os.path.exists(ranking_file):
        print(f"[pipeline] Error: Ranking file not found: {ranking_file}")
        return
    # Ranking file written (reduce console clutter)
    
    # Step 2: Extract BUY signals and analyze with limiter
    print(f"[pipeline] Extracting BUY signals for trend analysis...")
    try:
        # Extract only BUY signals from ranking file
        buy_signals_file = extract_buy_signals(ranking_file)
        print(f"Extracted {count_lines(buy_signals_file)} BUY signals for analysis")
        
        # Run limiter analysis on BUY signals only  
        from src.limiter import run_limiter_with_data
        from src.data_loader import StockDataLoader
        import logging
        
        # Suppress noisy data loader logs during limiter analysis
        logging.getLogger('src.data_loader').setLevel(logging.WARNING)
        
        # Load data for symbols in BUY signals
        symbols_data = {}
        loader = StockDataLoader()
        
        # Read symbols from BUY signals file
        buy_symbols = []
        with open(buy_signals_file, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2 and parts[0].isdigit():
                    buy_symbols.append(parts[1])
        
        # Load data for each symbol (no quality validation needed - already verified)
        for symbol in buy_symbols:
            try:
                df = loader.get_symbol_data(symbol, start_date=(datetime.now() - timedelta(days=180)).strftime('%Y-%m-%d'))
                if not df.empty:
                    symbols_data[symbol] = df
            except Exception as e:
                print(f"   Warning: Could not load data for {symbol}: {e}")
        
        loader.close()
        
        if symbols_data:
            temp_limiter_result = run_limiter_with_data(symbols_data, out_dir='results/reports')
            # Save the JSON result to a file for compatibility with existing code
            import json
            temp_limiter_json_path = 'results/reports/limiter.json'
            os.makedirs('results/reports', exist_ok=True)
            with open(temp_limiter_json_path, 'w') as f:
                json.dump(temp_limiter_result, f, indent=2)
            temp_limiter_json = temp_limiter_json_path
        else:
            raise Exception("No data loaded for BUY signals analysis")
        
        # Apply pre-filtering to remove bearish/consolidating stocks (single pass only)
        filtered_ranking_file = apply_trend_prefilter(buy_signals_file, temp_limiter_json)
        
    except Exception as e:
        print(f"[pipeline] BUY signal filtering failed: {e}")
        print("Continuing with original ranking...")
        filtered_ranking_file = ranking_file

    # Step 3: Decorrelate from pre-filtered signals
    print(f"[pipeline] Decorrelating from trend-filtered signals...")
    try:
        from src.decorrelate import run_decorrelate, load_config
        import tempfile
        import yaml
        import shutil
        
        # Load the decorrelate config and update the model name
        config = load_config('configs/decorrelate.yaml')
        # Don't override ranking_file in config - we'll pass filtered_ranking_file as override
        
        # Replace model name placeholder in output prefix
        if 'output' in config and 'prefix' in config['output']:
            config['output']['prefix'] = config['output']['prefix'].replace('{{model_name}}', model_name)
        
        # Write temporary config file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as temp_config:
            yaml.dump(config, temp_config)
            temp_config_path = temp_config.name
        
        try:
            out_txt, out_json = run_decorrelate(temp_config_path, filtered_ranking_file)
            print(f"De-correlated shortlist: {out_txt}")
            
            # Copy decorrelated results to where mithra.sh can read it
            vgmini_ranking_file = f"{model_name}_ranking.txt"  # For reports.py to find
            
            if os.path.exists(out_txt):
                shutil.copy2(out_txt, vgmini_ranking_file)  # Copy to vgmini root for reports.py
                print(f"Final ranking saved to: {vgmini_ranking_file}")
                ranking_file = vgmini_ranking_file  # Update for next steps
            
        finally:
            # Clean up temporary config file
            import os
            if os.path.exists(temp_config_path):
                os.unlink(temp_config_path)
                
    except Exception as e:
        print(f"[pipeline] Decorrelation failed: {e}")
        return
        
    # Step 3: Trail stops
    print(f"[pipeline] Computing trail stops...")
    try:
        from src.trailstop import run_trailstop
        trailstop_json = run_trailstop(out_txt, 'results/reports')
        # Trailstop analysis completed
        # Ensure we use the JSON file for chart rendering
        if trailstop_json.endswith('.csv'):
            trailstop_json = 'results/reports/trailstop.json'
    except Exception as e:
        print(f"[pipeline] Trail stop analysis failed: {e}")
        return
    
    # Step 4: Limiter analysis  
    print(f"[pipeline] Analyzing swing highs/lows...")
    try:
        from src.limiter import run_limiter_with_data
        from src.data_loader import StockDataLoader
        import logging
        
        # Suppress noisy data loader logs during limiter analysis
        logging.getLogger('src.data_loader').setLevel(logging.WARNING)
        
        # Load data for symbols in final ranking
        symbols_data = {}
        loader = StockDataLoader()
        
        # Read symbols from final ranking file
        final_symbols = []
        with open(out_txt, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2 and parts[0].isdigit():
                    final_symbols.append(parts[1])
        
        # Load data for each symbol (no quality validation needed - already verified)
        for symbol in final_symbols:
            try:
                df = loader.get_symbol_data(symbol, start_date=(datetime.now() - timedelta(days=180)).strftime('%Y-%m-%d'))
                if not df.empty:
                    symbols_data[symbol] = df
            except Exception as e:
                print(f"   Warning: Could not load data for {symbol}: {e}")
        
        loader.close()
        
        if symbols_data:
            limiter_result = run_limiter_with_data(symbols_data, out_dir='results/reports')
            # Save the JSON result to a file for compatibility with existing code
            limiter_json = 'results/reports/limiter.json'
        else:
            raise Exception("No data loaded for limiter analysis")
        # Limiter analysis completed
    except Exception as e:
        print(f"[pipeline] Limiter analysis failed: {e}")
        return
        
    # Step 5: Render charts
    print(f"[pipeline] Rendering charts...")
    try:
        from src.render_charts import render_charts_with_trailstop
        # Ensure reports directory exists
        os.makedirs('results/reports', exist_ok=True)
        # Parse symbols from the final ranking file (decorrelated)
        symbols = []
        if os.path.exists(ranking_file):
            with open(ranking_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and not line.startswith('De-correlated') and not line.startswith('Total'):
                        parts = line.split()
                        if len(parts) >= 2 and parts[0].isdigit():  # Must start with rank number
                            symbols.append(parts[1])  # Symbol is typically second column
        
        if symbols:
            # Pass both trailstop and limiter data for comprehensive chart overlays
            limiter_json = 'results/reports/limiter.json'
            render_charts_with_trailstop(symbols, 'results/reports', trailstop_json, ranking_file, limiter_json)
            # Charts rendered successfully
            if len(symbols) <= 10:  # Only show symbols if 10 or fewer
                print(f"   Symbols: {', '.join(symbols)}")
            # Top symbols already shown in confidence ordering
        else:
            print(f"[pipeline] No symbols found in ranking file: {ranking_file}")
    except Exception as e:
        print(f"[pipeline] Chart rendering failed: {e}")
        return
    
    # Step 6: Create model config reference for mithra.sh
    print(f"[pipeline] Preparing model configuration reference...")
    try:
        # Create a simple reference file pointing to the YAML config
        config_ref_file = f"results/{model_name}_config.yaml"
        yaml_config_path = f"configs/experiments/{model_name}.yaml"
        
        if os.path.exists(yaml_config_path):
            # Create symlink or copy to results for easier access by mithra.sh
            import shutil
            shutil.copy2(yaml_config_path, config_ref_file)
            # Config reference created
            print(f"Source config: {yaml_config_path}")
        else:
            print(f"Warning: Source config not found: {yaml_config_path}")
            
    except Exception as e:
        print(f"Warning: Config reference creation failed: {e}")
    
    # Step 7: Summary
    print(f"[pipeline] Pipeline completed successfully!")
    print(f"ğŸ“Š Final outputs for mithra.sh:")
    print(f"   ğŸ“‹ Ranking: {ranking_file} (decorrelated)")
    print(f"   ğŸ“ˆ Charts: results/reports/")
    print(f"   âš™ï¸  Model config: configs/experiments/{model_name}.yaml (editable)")
    print(f"   ğŸ›¡ï¸  Trailstop: results/reports/trailstop.json")
    print(f"   âš–ï¸  Limiter: results/reports/limiter.json")
    print(f"\nâœ… Ready for mithra.sh integration!")


def list_configs():
    """List available experiments from YAML (preferred) and legacy JSON configs."""
    from src.yaml_config import list_yaml_experiments

    # YAML experiments
    yaml_exps = list_yaml_experiments()
    print("ğŸ“‹ Available Experiments (YAML):")
    print("=" * 50)
    if yaml_exps:
        for name in sorted(yaml_exps.keys()):
            meta = yaml_exps[name]
            print(f"\nğŸ”§ {name}")
            print(f"   File: {os.path.relpath(meta['path'])}")
            print(f"   Description: {meta.get('description','No description')}")
    else:
        print("(none found under configs/experiments)")

    # Legacy JSON configs under results/configs for backward-compatibility
    config_dir = "results/configs"
    if os.path.exists(config_dir):
        configs = [f for f in os.listdir(config_dir) if f.endswith('.json')]
        if configs:
            print("\nğŸ“¦ Legacy Configurations (JSON):")
            print("=" * 50)
            for config_file in sorted(configs):
                config_path = os.path.join(config_dir, config_file)
                try:
                    with open(config_path, 'r') as f:
                        config_data = json.load(f)
                    name = config_data.get('name', config_file.replace('.json', ''))
                    description = config_data.get('description', 'No description')
                    symbols = config_data.get('symbols', [])
                    print(f"\nğŸ”§ {name}")
                    print(f"   File: {config_file}")
                    print(f"   Description: {description}")
                    if symbols:
                        print(f"   Symbols: {', '.join(symbols)}")
                except Exception as e:
                    print(f"\nâŒ {config_file} - Error reading: {e}")
    else:
        print("\n(no legacy JSON config directory found)")

def run_experiment(config_name: str, verbose: bool = False):
    """Run a single experiment by name.
    Priority: YAML in configs/experiments/<name>.yaml, else legacy results/configs/<name>.json
    """
    from src.yaml_config import load_merged_experiment_yaml

    print(f"ğŸ” DEBUG: Starting run_experiment for '{config_name}'")
    
    # Attempt YAML first
    yaml_path = os.path.join("configs", "experiments", f"{config_name}.yaml")
    json_path = f"results/configs/{config_name}.json"
    
    print(f"ğŸ” DEBUG: Checking YAML path: {yaml_path}")
    print(f"ğŸ” DEBUG: YAML exists: {os.path.exists(yaml_path)}")
    print(f"ğŸ” DEBUG: JSON path: {json_path}")
    print(f"ğŸ” DEBUG: JSON exists: {os.path.exists(json_path)}")

    if not os.path.exists(yaml_path):
        print(f"âŒ YAML config not found: {yaml_path}")
        print("ğŸ’¡ Use 'vgmini.py list' to see available experiments")
        return None
    
    try:
        config_data = load_merged_experiment_yaml(config_name)
        if not config_data:
            print(f"âŒ Failed to load config for {config_name}")
            return None
    except Exception as e:
        print(f"âŒ Error loading config: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
        return None
    
    if config_data is None:
        print(f"âŒ DEBUG: Final config_data is None - this should not happen!")
        return None

    print(f"ğŸš€ Running experiment: {config_name}")
    print("=" * 60)
    
    try:
        config = ExperimentConfig.from_dict(config_data)
        
        if verbose:
            print(f"ğŸ“Š Configuration Details:")
            print(f"   Symbols: {', '.join(config.symbols)}")
            print(f"   Date Range: {config.start_date} to {config.end_date}")
            print(f"   Features: {len(config.enabled_features)} enabled")
            print(f"   Confidence Threshold: {config.confidence_threshold}")
        
        # Save YAML snapshot if available
        try:
            import yaml
            os.makedirs(f"results/{config.name}", exist_ok=True)
            with open(f"results/{config.name}/config_snapshot.yaml", "w") as cf:
                yaml.safe_dump(config_data, cf, sort_keys=False)
        except Exception as _e:
            if verbose:
                print(f"âš ï¸  Failed to write config snapshot: {_e}")
        
        # Initialize framework with configuration overrides
        config_overrides = {
            'model_config': {
                'buy_threshold': config.buy_threshold,
                'sell_threshold': config.sell_threshold,
                'forecast_horizon': config.forecast_horizon,
                'test_size': config.test_size,
                'class_weight': config.class_weight,
                'penalty': config.penalty,
                'solver': config.solver,
                'max_iter': config.max_iter,
                'tol': config.tol,
                'model_type': config.model_type,
                'target_strategy': getattr(config, 'target_strategy', 'dual'),
                'n_estimators': config.n_estimators,
                'max_depth': config.max_depth,
                'learning_rate': config.learning_rate,
                'subsample': config.subsample,
                'colsample_bytree': config.colsample_bytree,
                'ema_periods': config.ema_periods,
                'volume_window': config.volume_window,
                'base_models': getattr(config, 'base_models', None),
                'weights': getattr(config, 'weights', None),
                # Equal-weight knobs
                'ew_correlation_signs': getattr(config, 'ew_correlation_signs', False),
                'ew_calibrate_prior': getattr(config, 'ew_calibrate_prior', False),
                'ew_activation_threshold': getattr(config, 'ew_activation_threshold', 0.0),
                'ew_temperature': getattr(config, 'ew_temperature', 1.0),
                'ew_buy_aggregation': getattr(config, 'ew_buy_aggregation', 'fraction'),
                'ew_sell_aggregation': getattr(config, 'ew_sell_aggregation', 'fraction')
            },
            'backtest_config': {
                'initial_capital': config.initial_capital,
                'commission': config.commission,
                'slippage': getattr(config, 'slippage', None),
                'confidence_threshold': config.confidence_threshold,
                'max_position_size': config.max_position_size,
                'threshold_strategy': getattr(config, 'threshold_strategy', 'absolute'),
                'threshold_percentile': getattr(config, 'threshold_percentile', 0.7),
                'threshold_window': getattr(config, 'threshold_window', 60),
                'auto_threshold_fallback': getattr(config, 'auto_threshold_fallback', True),
                'fallback_percentile': getattr(config, 'fallback_percentile', 0.7),
                'fallback_window': getattr(config, 'fallback_window', 60),
                'buy_consecutive_days': getattr(config, 'buy_consecutive_days', 1),
                'sell_consecutive_days': getattr(config, 'sell_consecutive_days', 1),
                'hysteresis_margin': getattr(config, 'hysteresis_margin', 0.0),
                'trade_cooldown_days': getattr(config, 'trade_cooldown_days', 0),
                'min_holding_days': getattr(config, 'min_holding_days', 0)
            },
            'technical_indicators': config.technical_indicators,
            'enabled_features': config.enabled_features,
            'name': config.name,  # Pass experiment name for special handling
            'analysis': config.analysis,
            'visualization': config.visualization
        }
        
        framework = TradingFramework(config_overrides=config_overrides)
        
        # Run experiment
        print("âš™ï¸  Starting experiment...")
        results = framework.run_experiment(
            symbols=config.symbols,
            start_date=config.start_date,
            end_date=config.end_date,
            experiment_name=config.name
        )
        
        # Close framework
        framework.close()
        
        # If the framework returned an error, report and exit early
        if results is None or ('error' in results):
            print("\n" + "=" * 60)
            print("âŒ EXPERIMENT FAILED")
            print("=" * 60)
            if results and 'error' in results:
                print(f"   Error: {results['error']}")
            print(f"\nğŸ’¾ Partial results saved to: results/{config_name}/ (if any)")
            return results
        
        # Print summary
        print("\n" + "=" * 60)
        print("âœ… EXPERIMENT COMPLETED SUCCESSFULLY!")
        print("=" * 60)
        
        print(f"\nğŸ“ˆ Key Results:")
        
        # Extract backtest results from the experiment results structure
        if 'backtest_results' in results:
            backtest_results = results['backtest_results']
            
            total_trades = 0
            avg_return = 0
            avg_sharpe = 0
            worst_drawdown = 0
            avg_volatility = 0
            avg_win_rate = 0
            symbol_count = 0
            
            print(f"   Individual Symbol Results:")
            for symbol, bt_results in backtest_results.items():
                if 'error' not in bt_results:
                    total_return = bt_results.get('total_return', 0)
                    sharpe = bt_results.get('sharpe_ratio', 0)
                    drawdown = bt_results.get('max_drawdown', 0)
                    volatility = bt_results.get('volatility', 0)
                    win_rate = bt_results.get('win_rate', 0)
                    trades = bt_results.get('total_trades', 0)
                    
                    print(f"     {symbol}: {total_return:.2%} return, {sharpe:.2f} Sharpe, {trades} trades")
                    
                    total_trades += trades
                    avg_return += total_return
                    avg_sharpe += sharpe
                    avg_volatility += volatility
                    avg_win_rate += win_rate
                    worst_drawdown = min(worst_drawdown, drawdown)
                    symbol_count += 1
            
            if symbol_count > 0:
                avg_return /= symbol_count
                avg_sharpe /= symbol_count
                avg_volatility /= symbol_count
                avg_win_rate /= symbol_count
                
                print(f"   Average Return: {avg_return:.2%}")
                print(f"   Average Sharpe: {avg_sharpe:.2f}")
                print(f"   Average Volatility: {avg_volatility:.2%}")
                print(f"   Worst Drawdown: {worst_drawdown:.2%}")
                print(f"   Average Win Rate: {avg_win_rate:.2%}")
                print(f"   Total Trades: {total_trades}")
            else:
                print(f"   No valid backtest results found")
        else:
            print(f"   No backtest results found in experiment output")
        
        print(f"\nğŸ’¾ Results saved to: results/{config_name}/")
        
        return results
        
    except Exception as e:
        print(f"âŒ Error running experiment: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
        return None

def compare_all_experiments(verbose: bool = False):
    """Compare all completed experiments in results/ folder"""
    results_dir = "results"
    if not os.path.exists(results_dir):
        print("âŒ No results directory found")
        return
    
    # Find all experiment directories with results
    experiment_dirs = []
    for item in os.listdir(results_dir):
        experiment_path = os.path.join(results_dir, item)
        if os.path.isdir(experiment_path) and item != "configs":
            # Check if it has experiment results
            results_file = os.path.join(experiment_path, "experiment_results.json")
            if os.path.exists(results_file):
                experiment_dirs.append(item)
    
    if len(experiment_dirs) < 2:
        print(f"âŒ Found only {len(experiment_dirs)} completed experiments. Need at least 2 to compare.")
        print("ğŸ’¡ Run some experiments first:")
        print("   python vgmini.py dual_signal")
        print("   python vgmini.py dual_signal_v2")
        return
    
    print(f"ğŸ”¬ Comparing {len(experiment_dirs)} completed experiments:")
    for exp in sorted(experiment_dirs):
        print(f"   â€¢ {exp}")
    print("=" * 60)
    
    # Load results from each experiment
    results = {}
    for exp_name in experiment_dirs:
        results_file = os.path.join(results_dir, exp_name, "experiment_results.json")
        try:
            with open(results_file, 'r') as f:
                results[exp_name] = json.load(f)
            print(f"âœ… Loaded results for {exp_name}")
        except Exception as e:
            print(f"âŒ Failed to load results for {exp_name}: {e}")
            continue
    
    if len(results) < 2:
        print("âŒ Failed to load enough experiment results for comparison")
        return
    
    # Perform comparison
    compare_loaded_results(results, verbose)

def compare_experiments(config_names: List[str], verbose: bool = False):
    """Compare multiple experiments by running them"""
    if len(config_names) < 2:
        print("âŒ Need at least 2 experiments to compare")
        return
    
    print(f"ğŸ”¬ Comparing {len(config_names)} experiments:")
    for name in config_names:
        print(f"   â€¢ {name}")
    print("=" * 60)
    
    # Run all experiments
    results = {}
    for config_name in config_names:
        print(f"\nğŸƒ Running {config_name}...")
        result = run_experiment(config_name, verbose=False)
        if result is None:
            print(f"âŒ Failed to run {config_name}")
            return
        results[config_name] = result
    
    # Perform comparison
    compare_loaded_results(results, verbose)
    
    return results

def compare_loaded_results(results: Dict[str, Any], verbose: bool = False):
    """Compare loaded experiment results"""
    config_names = list(results.keys())
    
    # Compare results
    print(f"\n{'='*80}")
    print("ğŸ“Š EXPERIMENT COMPARISON")
    print(f"{'='*80}")
    
    # Metrics to compare
    metrics = [
        ('total_return', 'Total Return', '%'),
        ('sharpe_ratio', 'Sharpe Ratio', ''),
        ('max_drawdown', 'Max Drawdown', '%'),
        ('volatility', 'Volatility', '%'),
        ('total_trades', 'Total Trades', ''),
        ('win_rate', 'Win Rate', '%')
    ]
    
    # Create comparison table
    print(f"\n{'Metric':<20}", end='')
    for name in config_names:
        print(f"{name:<15}", end='')
    print("Best")
    print("-" * (20 + 15 * len(config_names) + 10))
    
    comparison_data = []
    
    for metric, display_name, unit in metrics:
        print(f"{display_name:<20}", end='')
        
        values = []
        for name in config_names:
            # Extract metrics from backtest_results structure
            value = 0
            if 'backtest_results' in results[name]:
                backtest_results = results[name]['backtest_results']
                total_value = 0
                count = 0
                
                for symbol, bt_results in backtest_results.items():
                    symbol_value = bt_results.get(metric, 0)
                    
                    if metric in ['total_return', 'sharpe_ratio', 'max_drawdown', 'volatility', 'win_rate']:
                        total_value += symbol_value
                        count += 1
                    elif metric == 'total_trades':
                        # Use the total_trades field directly
                        total_value += bt_results.get('total_trades', 0)
                        count = 1  # Don't average trades, sum them
                
                if count > 0:
                    if metric == 'total_trades':
                        value = total_value  # Sum of trades
                    else:
                        value = total_value / count  # Average of other metrics
            
            values.append(value)
            
            if unit == '%':
                print(f"{value:.2%}".ljust(15), end='')
            elif metric == 'sharpe_ratio':
                print(f"{value:.3f}".ljust(15), end='')
            else:
                print(f"{value:.0f}".ljust(15), end='')
        
        # Determine best (lowest for drawdown and volatility, highest for others)
        if metric in ['max_drawdown', 'volatility']:
            # For risk metrics, lower absolute values are better
            abs_values = [abs(v) for v in values]
            best_idx = abs_values.index(min(abs_values)) if any(v != 0 for v in abs_values) else 0
        else:
            best_idx = values.index(max(values)) if any(v != 0 for v in values) else 0
        
        print(f"{config_names[best_idx]}")
        
        comparison_data.append({
            'metric': metric,
            'values': dict(zip(config_names, values)),
            'best': config_names[best_idx]
        })
    
    # Feature analysis
    print(f"\n{'='*80}")
    print("ğŸ” FEATURE ANALYSIS")
    print(f"{'='*80}")
    
    for config_name in config_names:
        # Try to get feature importance from model_results or shap_analysis
        buy_features = None
        # Label equal-weight model if detected
        model_type_label = results[config_name].get('config', {}).get('model_config', {}).get('model_type', '')
        if str(model_type_label).lower() == 'equal_weight':
            print(f"\nâ„¹ï¸  {config_name} uses Equal-Weight model (uniform coefficients by design; SHAP/LIME may vary).")
        
        if 'model_results' in results[config_name] and 'buy_model' in results[config_name]['model_results']:
            buy_features = results[config_name]['model_results']['buy_model'].get('feature_importance', {})
        elif 'shap_analysis' in results[config_name] and 'comprehensive_analysis' in results[config_name]['shap_analysis']:
            comp_analysis = results[config_name]['shap_analysis']['comprehensive_analysis']
            if 'buy_model' in comp_analysis and 'linear_coefficients' in comp_analysis['buy_model']:
                buy_features = comp_analysis['buy_model']['linear_coefficients']
        
        if buy_features:
            print(f"\nğŸ“‹ {config_name} - Top 5 Buy Model Features:")
            # Filter out zero values and RSI features for dual_signal_v2
            filtered_features = {}
            for feature, importance in buy_features.items():
                if abs(importance) > 1e-6:  # Filter out essentially zero values
                    # For dual_signal_v2, exclude RSI features
                    if config_name == 'dual_signal_v2' and 'rsi' in feature.lower():
                        continue
                    filtered_features[feature] = importance
            
            buy_sorted = sorted(filtered_features.items(), key=lambda x: abs(x[1]), reverse=True)[:5]
            
            for i, (feature, importance) in enumerate(buy_sorted):
                direction = "ğŸ“ˆ" if importance > 0 else "ğŸ“‰"
                print(f"   {i+1}. {feature}: {importance:.4f} {direction}")
        else:
            print(f"\nğŸ“‹ {config_name} - No feature importance data available")
    
    # Create visualization
    create_comparison_visualization(results, config_names, comparison_data)
    
    # Summary
    print(f"\n{'='*80}")
    print("ğŸ¯ SUMMARY")
    print(f"{'='*80}")
    
    # Count wins for each experiment
    wins = {name: 0 for name in config_names}
    for data in comparison_data:
        wins[data['best']] += 1
    
    print(f"\nğŸ† Performance Ranking:")
    sorted_wins = sorted(wins.items(), key=lambda x: x[1], reverse=True)
    for i, (name, win_count) in enumerate(sorted_wins):
        print(f"   {i+1}. {name}: {win_count}/{len(metrics)} metrics won")
    
    # Write comparison summary to results/summary/ (CSV + HTML)
    import os, csv
    summary_dir = os.path.join('results', 'summary')
    os.makedirs(summary_dir, exist_ok=True)
    
    # CSV with metrics table
    csv_path = os.path.join(summary_dir, 'comparison_summary.csv')
    fieldnames = ['metric'] + config_names + ['best']
    with open(csv_path, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in comparison_data:
            out = {'metric': row['metric']}
            out.update(row['values'])
            out['best'] = row['best']
            writer.writerow(out)
    
    # Simple HTML table
    html_path = os.path.join(summary_dir, 'comparison_summary.html')
    try:
        rows = []
        rows.append('<html><head><meta charset="utf-8"><style>table{border-collapse:collapse}th,td{border:1px solid #ccc;padding:6px}</style></head><body>')
        rows.append(f'<h2>Experiment Comparison</h2>')
        rows.append('<table>')
        header = '<tr>' + ''.join([f'<th>{h}</th>' for h in fieldnames]) + '</tr>'
        rows.append(header)
        for row in comparison_data:
            tds = [f'<td>{row["metric"]}</td>']
            for name in config_names:
                val = row['values'][name]
                cell = f"{val:.4f}" if isinstance(val, (int, float)) else f"{val}"
                tds.append(f'<td>{cell}</td>')
            tds.append(f'<td>{row["best"]}</td>')
            rows.append('<tr>' + ''.join(tds) + '</tr>')
        rows.append('</table>')
        rows.append('</body></html>')
        with open(html_path, 'w') as f:
            f.write('\n'.join(rows))
        print(f"\nğŸ“‘ Comparison summaries saved to: {summary_dir}")
    except Exception as e:
        print(f"âš ï¸  Failed to write HTML summary: {e}")

def create_comparison_visualization(results: Dict[str, Any], config_names: List[str], comparison_data: List[Dict]):
    """Create comparison visualization"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(f'Experiment Comparison: {" vs ".join(config_names)}', fontsize=16, fontweight='bold')
    
    # 1. Performance metrics
    ax1 = axes[0, 0]
    metrics = ['total_return', 'sharpe_ratio', 'win_rate']
    x = np.arange(len(metrics))
    width = 0.8 / len(config_names)
    
    for i, name in enumerate(config_names):
        values = [next(d['values'][name] for d in comparison_data if d['metric'] == m) for m in metrics]
        ax1.bar(x + i * width, values, width, label=name, alpha=0.8)
    
    ax1.set_xlabel('Metrics')
    ax1.set_ylabel('Value')
    ax1.set_title('Key Performance Metrics')
    ax1.set_xticks(x + width * (len(config_names) - 1) / 2)
    ax1.set_xticklabels(metrics)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Portfolio comparison (if available)
    ax2 = axes[0, 1]
    portfolio_plotted = False
    for name in config_names:
        if 'portfolio_history' in results[name]:
            portfolio = results[name]['portfolio_history']
            ax2.plot(portfolio['date'], portfolio['total_value'], label=name, linewidth=2)
            portfolio_plotted = True
    
    if portfolio_plotted:
        ax2.set_title('Portfolio Value Over Time')
        ax2.set_ylabel('Portfolio Value ($)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
    else:
        ax2.text(0.5, 0.5, 'Portfolio history not available', ha='center', va='center', transform=ax2.transAxes)
        ax2.set_title('Portfolio Value Over Time')
    
    # 3. Risk metrics
    ax3 = axes[1, 0]
    risk_metrics = ['max_drawdown', 'volatility']
    x_risk = np.arange(len(risk_metrics))
    
    for i, name in enumerate(config_names):
        values = [abs(next(d['values'][name] for d in comparison_data if d['metric'] == m)) for m in risk_metrics]
        ax3.bar(x_risk + i * width, values, width, label=name, alpha=0.8)
    
    ax3.set_xlabel('Risk Metrics')
    ax3.set_ylabel('Value (absolute)')
    ax3.set_title('Risk Metrics Comparison')
    ax3.set_xticks(x_risk + width * (len(config_names) - 1) / 2)
    ax3.set_xticklabels(risk_metrics)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. Trade statistics
    ax4 = axes[1, 1]
    trade_values = [next(d['values'][name] for d in comparison_data if d['metric'] == 'total_trades') for name in config_names]
    
    ax4.bar(config_names, trade_values, alpha=0.8)
    ax4.set_xlabel('Experiments')
    ax4.set_ylabel('Number of Trades')
    ax4.set_title('Trading Activity Comparison')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save comparison plot (overwrite previous)
    comparison_path = "results/latest_comparison.png"
    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š Comparison plot saved to: {comparison_path}")

def train_model(model_name: str, verbose: bool = False):
    """Train a model using the specified experiment configuration"""
    print(f"ğŸ‹ï¸  Training model: {model_name}")
    print("=" * 60)
    
    # Load YAML config only
    yaml_config_path = f"configs/experiments/{model_name}.yaml"
    if not os.path.exists(yaml_config_path):
        print(f"âŒ YAML config not found: {yaml_config_path}")
        print("ğŸ’¡ Use 'vgmini.py list' to see available experiments")
        return
    
    try:
        from src.yaml_config import load_merged_experiment_yaml
        from src.experiment_configs import ExperimentConfig
        from src.main_framework import TradingFramework
        
        config_data = load_merged_experiment_yaml(model_name)
        if not config_data:
            print(f"âŒ Failed to load config for {model_name}")
            return
            
        config = ExperimentConfig.from_dict(config_data)
        
        if verbose:
            print(f"ğŸ“Š Training Configuration:")
            print(f"   Model Type: {config.model_type}")
            print(f"   Symbols: {len(config.symbols)} symbols")
            print(f"   Date Range: {config.start_date} to {config.end_date}")
            print(f"   Features: {len(config.enabled_features)} enabled")
        
        # Initialize framework for training only
        config_overrides = {
            'model_config': {
                'buy_threshold': config.buy_threshold,
                'sell_threshold': config.sell_threshold,
                'forecast_horizon': config.forecast_horizon,
                'test_size': config.test_size,
                'class_weight': config.class_weight,
                'penalty': config.penalty,
                'solver': config.solver,
                'max_iter': config.max_iter,
                'tol': config.tol,
                'model_type': config.model_type,
                'target_strategy': getattr(config, 'target_strategy', 'dual'),
                'n_estimators': config.n_estimators,
                'max_depth': config.max_depth,
                'learning_rate': config.learning_rate,
                'subsample': config.subsample,
                'colsample_bytree': config.colsample_bytree,
                'ema_periods': config.ema_periods,
                'volume_window': config.volume_window,
                'base_models': getattr(config, 'base_models', None),
                'weights': getattr(config, 'weights', None),
                # Equal-weight knobs - CRITICAL FOR EQUAL WEIGHT MODEL
                'ew_correlation_signs': getattr(config, 'ew_correlation_signs', False),
                'ew_calibrate_prior': getattr(config, 'ew_calibrate_prior', False),
                'ew_activation_threshold': getattr(config, 'ew_activation_threshold', 0.0),
                'ew_temperature': getattr(config, 'ew_temperature', 1.0),
                'ew_buy_aggregation': getattr(config, 'ew_buy_aggregation', 'fraction'),
                'ew_sell_aggregation': getattr(config, 'ew_sell_aggregation', 'fraction')
            },
            'backtest_config': {
                'initial_capital': config.initial_capital,
                'commission': config.commission,
                'slippage': getattr(config, 'slippage', None),
                'confidence_threshold': config.confidence_threshold,
                'max_position_size': config.max_position_size,
                'threshold_strategy': getattr(config, 'threshold_strategy', 'absolute'),
                'threshold_percentile': getattr(config, 'threshold_percentile', 0.7),
                'threshold_window': getattr(config, 'threshold_window', 60),
                'auto_threshold_fallback': getattr(config, 'auto_threshold_fallback', True),
                'fallback_percentile': getattr(config, 'fallback_percentile', 0.7),
                'fallback_window': getattr(config, 'fallback_window', 60),
                'buy_consecutive_days': getattr(config, 'buy_consecutive_days', 1),
                'sell_consecutive_days': getattr(config, 'sell_consecutive_days', 1),
                'hysteresis_margin': getattr(config, 'hysteresis_margin', 0.0),
                'trade_cooldown_days': getattr(config, 'trade_cooldown_days', 0),
                'min_holding_days': getattr(config, 'min_holding_days', 0)
            },
            'technical_indicators': config.technical_indicators,
            'enabled_features': config.enabled_features,
            'name': config.name,
            'analysis': config.analysis,
            'visualization': config.visualization
        }
        
        framework = TradingFramework(config_overrides=config_overrides)
        
        print("âš™ï¸  Training and testing model...")
        # Run full experiment (training + testing)
        results = framework.run_experiment(
            symbols=config.symbols,
            start_date=config.start_date,
            end_date=config.end_date,
            experiment_name=config.name
        )
        
        framework.close()
        
        if results and 'error' not in results:
            print("âœ… Model training completed successfully!")
            print(f"ğŸ’¾ Model saved to: results/{model_name}/models/trained_model.pkl")
        else:
            print("âŒ Model training failed")
            
    except Exception as e:
        print(f"âŒ Error training model: {e}")
        if verbose:
            import traceback
            traceback.print_exc()


def rank_symbols(model_name: str, top_n: int = 10, verbose: bool = False, threshold: float = None, sort_by: str = 'er', downside_k: float = 0.7, quiet: bool = False):
    """Rank all database symbols by buy potential using a previously trained model.
    
    Expects a trained model at results/{model_name.lower()}/models/trained_model.pkl
    and uses the same feature configuration as the experiment to compute features
    up to the latest available date per symbol.
    """
    # Ranking symbols (reduce visual clutter)

    import json
    from datetime import timedelta

    # Resolve paths and config
    exp_name = model_name.lower()
    model_path = f"results/{exp_name}/models/trained_model.pkl"
    config_path = f"results/configs/{exp_name}.json"

    if not os.path.exists(model_path):
        print(f"âŒ Trained model not found: {model_path}")
        print(f"ğŸ’¡ Run the experiment first: python vgmini.py {exp_name}")
        return

    # Load experiment config if available (to match training-time feature settings)
    exp_config = None
    
    # Load YAML experiment config only
    yaml_config_path = f"configs/experiments/{exp_name}.yaml"
    if os.path.exists(yaml_config_path):
        try:
            from src.yaml_config import load_merged_experiment_yaml
            exp_config = load_merged_experiment_yaml(exp_name)
        except Exception as e:
            if verbose:
                print(f"âš ï¸  Failed to load YAML experiment config ({yaml_config_path}): {e}")
    else:
        print(f"âŒ YAML config not found: {yaml_config_path}")
        print("ğŸ’¡ Use 'vgmini.py list' to see available experiments")
        return

    try:
        import numpy as np
        import pandas as pd
        from src.data_loader import StockDataLoader
        from src.feature_engineering import FeatureEngineer
        from src.ml_models import StockSignalPredictor
        from src.config import model_config as global_model_config
        import logging, time

        # Reduce noisy INFO logs from data loader during ranking
        logging.getLogger('src.data_loader').setLevel(logging.WARNING)
        logging.getLogger('sqlalchemy').setLevel(logging.WARNING)

        # Sync key model config parameters with experiment for feature calc (EMA, volume window, horizons)
        if exp_config:
            if 'ema_periods' in exp_config:
                global_model_config.ema_periods = exp_config['ema_periods']
            if 'volume_window' in exp_config:
                global_model_config.volume_window = exp_config['volume_window']
            if 'forecast_horizon' in exp_config:
                global_model_config.forecast_horizon = exp_config['forecast_horizon']
            if 'buy_threshold' in exp_config:
                global_model_config.buy_threshold = exp_config['buy_threshold']
            if 'sell_threshold' in exp_config:
                global_model_config.sell_threshold = exp_config['sell_threshold']

        # Load model
        predictor = StockSignalPredictor()
        predictor.load_models(model_path)
        # Model loaded successfully

        # Determine threshold to use
        if threshold is None:
            if isinstance(exp_config, dict) and 'confidence_threshold' in exp_config:
                threshold = float(exp_config['confidence_threshold'])
            else:
                print(f"âŒ No confidence_threshold found in experiment config for {model_name}")
                print(f"ğŸ’¡ Add 'confidence_threshold' to the model config or pass --threshold explicitly")
                print(f"ğŸ’¡ For equal_weight models, try values like 0.3-0.4")
                print(f"ğŸ’¡ For logistic models, try values like 0.5-0.7")
                return

        # Load universe of symbols from DB
        loader = StockDataLoader()
        symbols_df = loader.get_symbols()
        symbols_to_rank = symbols_df['symbol'].tolist()
        if not symbols_to_rank:
            print("âŒ No symbols found in the database")
            return

        # Determine a reasonable lookback window for feature calc
        # Use max EMA + 2x horizon + buffer
        max_ema = max(getattr(global_model_config, 'ema_periods', [9]))
        horizon = int(getattr(global_model_config, 'forecast_horizon', 5))
        lookback_days = int(max(260, max_ema + 2 * horizon + 50))

        # Determine end date from DB summary for daily interval only
        summary = loader.get_data_summary(interval='1d')
        end_ts = summary.get('date_range', {}).get('end_date', None)
        if end_ts is None:
            print("âŒ Could not determine latest date from database")
            return
        end_date = pd.to_datetime(end_ts).normalize()
        start_date = (end_date - pd.Timedelta(days=lookback_days)).date().isoformat()
        end_date_str = end_date.date().isoformat()

        # Feature engineer with experiment settings (for RSI/MACD/etc)
        # Pass the raw experiment dict so _get_config_value picks overrides
        fe = FeatureEngineer(config=exp_config if isinstance(exp_config, dict) else None)

        symbol_scores = []
        processed = 0
        successful_predictions = 0

        total = len(symbols_to_rank)
        # Starting analysis (console output reduced)
        start_time = time.time()

        def print_progress():
            import os, sys
            plain = os.getenv('VGM_PLAIN_PROGRESS', '') == '1' or not sys.stdout.isatty()
            elapsed = time.time() - start_time
            eta = (elapsed / processed * (total - processed)) if processed else 0
            if plain:
                # Throttle plain prints to avoid spam
                step = max(1, total // 50)
                if (processed % step == 0) or (processed == total):
                    progress = int(30 * processed / total) if total else 30
                    bar = 'â–ˆ' * progress + 'â–‘' * (30 - progress)
                    print(f" [{bar}] {processed}/{total} | ok {successful_predictions} | ETA {int(eta)}s")
            else:
                progress = int(30 * processed / total) if total else 30
                bar = 'â–ˆ' * progress + 'â–‘' * (30 - progress)
                print(f"\r [{bar}] {processed}/{total} | ok {successful_predictions} | ETA {int(eta)}s", end='', flush=True)

        for symbol in symbols_to_rank:
            try:
                df = loader.get_symbol_data(symbol, start_date=start_date, end_date=end_date_str)
                if df is None or df.empty:
                    if verbose:
                        print(f"âš ï¸  {symbol}: No data in range {start_date}..{end_date_str}")
                    continue

                # Ensure chronological order
                df = df.sort_values('datetime').reset_index(drop=True)

                # Compute features WITHOUT creating targets or dropping the last horizon rows
                fdf = df.copy()
                # Core indicators
                fdf = fe.calculate_ema(fdf, global_model_config.ema_periods)
                fdf = fe.calculate_macd(fdf)
                fdf = fe.calculate_heikin_ashi(fdf)
                fdf = fe.calculate_volume_indicators(fdf)
                fdf = fe.calculate_additional_indicators(fdf)
                fdf = fe.create_crossover_signals(fdf)

                # Take the last row as the current state
                last_row = fdf.iloc[[-1]]  # keep DataFrame shape

                # Build feature vector in the EXACT order used by the model
                feature_names = predictor.feature_names
                missing = [c for c in feature_names if c not in last_row.columns]
                if missing:
                    # If some features are missing (config mismatch), fill with 0s
                    for c in missing:
                        last_row[c] = 0.0
                    # Reorder after adding
                    last_row = last_row

                X = last_row[feature_names].fillna(0).values

                # Predict signals
                signals = predictor.predict_signals(X, confidence_threshold=threshold)
                buy_proba = float(signals['buy_probability'][0])
                sell_proba = float(signals['sell_probability'][0])
                buy_signal = int(signals['buy_signals'][0])
                margin = buy_proba - float(threshold)

                # Expected growth proxy: p*target - (1-p)*k*downside_sigma
                # Use 60-bar downside stdev of daily returns as proxy
                recent = df.tail(60).copy() if len(df) >= 60 else df.copy()
                if len(recent) >= 2:
                    recent['ret'] = recent['close'].pct_change()
                    downside = recent['ret'].clip(upper=0).abs()
                    downside_sigma = float(downside.std(skipna=True) or 0.0)
                    
                    # Calculate stock-specific downside risk multiplier based on volatility regime
                    overall_vol = recent['ret'].std()
                    if overall_vol > 0:
                        # Higher volatility stocks get higher downside risk multipliers
                        stock_downside_k = min(1.5, max(0.3, 0.7 + (overall_vol - 0.02) * 10))
                    else:
                        stock_downside_k = 0.7
                else:
                    downside_sigma = 0.0
                    stock_downside_k = 0.7
                
                target_gain = float(getattr(global_model_config, 'buy_threshold', 0.01))
                er = buy_proba * target_gain - (1.0 - buy_proba) * stock_downside_k * downside_sigma
                
                # DEBUG: ER calculations verified - remove when no longer needed

                symbol_scores.append({
                    'symbol': symbol,
                    'buy_score': buy_proba,
                    'sell_score': sell_proba,
                    'expected_return': er,
                    'downside_sigma': downside_sigma,
                    'margin': margin,
                    'buy_signal': buy_signal,
                    'latest_price': float(last_row['close'].iloc[0]),
                    'latest_date': str(last_row['datetime'].iloc[0].date()) if 'datetime' in last_row else end_date_str
                })
                successful_predictions += 1

                if verbose:
                    icon = "ğŸš€" if buy_signal else "â¸ï¸"
                    print(f"{icon} {symbol}: score={buy_proba:.3f} price=${float(last_row['close'].iloc[0]):.2f}")

            except Exception as e:
                if verbose:
                    print(f"âŒ {symbol}: {e}")
                continue
            finally:
                processed += 1
                print_progress()

        # Finish progress line
        print()

        if not symbol_scores:
            print("âŒ No symbols could be analyzed successfully")
            return

        # Sort and display top N
        if sort_by.lower() in ['er', 'expected_return']:
            symbol_scores.sort(key=lambda x: x.get('expected_return', 0.0), reverse=True)
        else:
            symbol_scores.sort(key=lambda x: x['buy_score'], reverse=True)
        top = min(top_n, len(symbol_scores))

        # Prepare a consolidated text output for file
        lines = []
        sort_desc = 'Expected Return' if sort_by.lower() in ['er','expected_return'] else 'Buy Score'
        header_main = f"\nğŸ† TOP {top} BUY OPPORTUNITIES (threshold={threshold:.2f}, sorted by {sort_desc})"
        lines.append(header_main)
        lines.append("=" * 90)
        lines.append(f"{'Rank':<4} {'Symbol':<8} {'Score':<8} {'Margin':<8} {'Signal':<8} {'Price':<10} {'Date':<12}")
        lines.append("-" * 90)

        if not quiet:
            print(header_main)
            print("=" * 90)
            print(f"{'Rank':<4} {'Symbol':<8} {'Score':<8} {'Margin':<8} {'Signal':<8} {'Price':<10} {'Date':<12}")
            print("-" * 90)

            for i, data in enumerate(symbol_scores[:top]):
                signal_text = "BUY" if data['buy_signal'] else "HOLD"
                signal_icon = "ğŸš€" if data['buy_signal'] else "â¸ï¸"
                line = f"{i+1:<4} {data['symbol']:<8} {data['buy_score']:.3f}    {data['margin']:.3f}   {signal_icon} {signal_text:<6} ${data['latest_price']:<8.2f} {data['latest_date']}"
                print(line)
                lines.append(line)

            # Additional table: only symbols priced > $20
            filtered_scores = [s for s in symbol_scores if s['latest_price'] > 20]
            if filtered_scores:
                # Already sorted by buy_score above, but ensure ordering after filtering
                filtered_scores.sort(key=lambda x: x['buy_score'], reverse=True)
                topf = min(top_n, len(filtered_scores))
                header_f = f"\nğŸ† TOP {topf} BUY OPPORTUNITIES (Price > $20, threshold={threshold:.2f})"
                print(header_f)
                print("=" * 90)
                print(f"{'Rank':<4} {'Symbol':<8} {'Score':<8} {'Margin':<8} {'Signal':<8} {'Price':<10} {'Date':<12}")
                print("-" * 90)
                lines.append(header_f)
                lines.append("=" * 90)
                lines.append(f"{'Rank':<4} {'Symbol':<8} {'Score':<8} {'Margin':<8} {'Signal':<8} {'Price':<10} {'Date':<12}")
                lines.append("-" * 90)
                for i, data in enumerate(filtered_scores[:topf]):
                    signal_text = "BUY" if data['buy_signal'] else "HOLD"
                    signal_icon = "ğŸš€" if data['buy_signal'] else "â¸ï¸"
                    line = f"{i+1:<4} {data['symbol']:<8} {data['buy_score']:.3f}    {data['margin']:.3f}   {signal_icon} {signal_text:<6} ${data['latest_price']:<8.2f} {data['latest_date']}"
                    print(line)
                    lines.append(line)
        else:
            # In quiet mode, just add the data to lines for file output
            for i, data in enumerate(symbol_scores[:top]):
                signal_text = "BUY" if data['buy_signal'] else "HOLD"
                signal_icon = "ğŸš€" if data['buy_signal'] else "â¸ï¸"
                line = f"{i+1:<4} {data['symbol']:<8} {data['buy_score']:.3f}    {data['margin']:.3f}   {signal_icon} {signal_text:<6} ${data['latest_price']:<8.2f} {data['latest_date']}"
                lines.append(line)

        # Add full ranked list of ALL symbols to the file output (not printed to console)
        lines.append("")
        sort_desc = 'Expected Return' if sort_by.lower() in ['er','expected_return'] else 'Buy Score'
        lines.append(f"ğŸ“‹ ALL SYMBOLS (sorted by {sort_desc}, threshold={threshold:.2f})")
        lines.append("=" * 90)
        lines.append(f"{'Rank':<4} {'Symbol':<8} {'Score':<8} {'Margin':<8} {'Signal':<8} {'Price':<10} {'Date':<12}")
        lines.append("-" * 90)
        for i, data in enumerate(symbol_scores):
            signal_text = "BUY" if data['buy_signal'] else "HOLD"
            signal_icon = "ğŸš€" if data['buy_signal'] else "â¸ï¸"
            line = f"{i+1:<4} {data['symbol']:<8} {data['buy_score']:.3f}    {data['margin']:.3f}   {signal_icon} {signal_text:<6} ${data['latest_price']:<8.2f} {data['latest_date']}"
            lines.append(line)

        # Summary stats written to file only (reduce console clutter)

        # Save consolidated output to file in project root
        # Write to results directory to keep vgmini root clean
        results_dir = "results"
        os.makedirs(results_dir, exist_ok=True)
        out_path = os.path.join(results_dir, f"{exp_name}_ranking.txt")
        try:
            with open(out_path, 'w') as f:
                f.write("\n".join(lines))
                f.write("\n\nğŸ“Š Summary:\n")
                f.write(f"   Symbols in DB: {len(symbols_to_rank)}\n")
                f.write(f"   Processed: {processed}\n")
                f.write(f"   Successful predictions: {successful_predictions}\n")
                f.write(f"   Buy signals flagged: {sum(1 for s in symbol_scores if s['buy_signal'])}\n")
                f.write(f"   Model: {model_name}\n")
                f.write(f"   Threshold: {threshold:.2f}\n")
            print(f"\nğŸ’¾ Ranking saved to: {out_path}")
        except Exception as fe:
            print(f"âš ï¸  Failed to write ranking file: {fe}")

    except Exception as e:
        print(f"âŒ Error during ranking: {e}")
        if verbose:
            import traceback
            traceback.print_exc()

def main():
    """Main CLI interface"""
    parser = argparse.ArgumentParser(
        description="VGMini Trading Framework - Professional Trading Strategy Backtesting",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  vgmini.py list                              # List available configurations
  vgmini.py train --model equal_weight_dual   # Train and test model
  vgmini.py rank equal_weight_dual            # Rank symbols using trained model
  vgmini.py pipeline --model equal_weight_dual # Run complete pipeline
  vgmini.py equal_weight_dual                 # Run full experiment (train+test)
  vgmini.py compare                           # Compare all completed experiments
        """
    )
    
    parser.add_argument('command', nargs='?', help='Command: list, train, rank, pipeline, compare, or experiment name')
    parser.add_argument('model_or_arg', nargs='?', help='Model name or additional argument')
    parser.add_argument('--model', type=str, help='Model name for train/test/pipeline commands')
    parser.add_argument('--top', type=int, default=10, help='Number of top symbols (for rank/pipeline)')
    parser.add_argument('--threshold', type=float, help='Override confidence threshold for ranking')
    parser.add_argument('--verbose', '-v', action='store_true', help='Enable verbose output')
    parser.add_argument('--quiet', '-q', action='store_true', help='Suppress verbose ranking output (for pipeline use)')
    
    args = parser.parse_args()
    
    # Framework initialized
    # Framework header reduced
    
    if not args.command:
        parser.print_help()
        return
    
    if args.command == 'list':
        list_configs()
    
    elif args.command == 'train':
        model_name = args.model or args.model_or_arg
        if not model_name:
            print("âŒ Model name required for training")
            print("ğŸ’¡ Example: vgmini.py train --model equal_weight_dual")
            return
        train_model(model_name, verbose=args.verbose)
    
    elif args.command == 'rank':
        model_name = args.model_or_arg
        if not model_name:
            print("âŒ Model name required for ranking")
            print("ğŸ’¡ Example: vgmini.py rank equal_weight_dual")
            return
        rank_symbols(model_name, top_n=args.top, verbose=args.verbose, threshold=args.threshold, quiet=args.quiet)
    
    elif args.command == 'pipeline':
        model_name = args.model
        if not model_name:
            print("âŒ Model name required for pipeline")
            print("ğŸ’¡ Example: vgmini.py pipeline --model equal_weight_dual")
            return
        run_pipeline(model_name)
    
    elif args.command == 'compare':
        compare_all_experiments(verbose=args.verbose)
    
    else:
        # Treat as experiment name (run full experiment: train + test)
        run_experiment(args.command, verbose=args.verbose)

if __name__ == "__main__":
    main()
```

RESPONSE:
## Analysis

### Overcomplication & Redundancy

1. **Self-managing virtual environment**: The script attempts to detect/create/restart in a venv, which is anti-pattern for a CLI tool. This adds ~50 lines of brittle subprocess logic that should be handled by the user/environment manager.

2. **Dual config systems**: Maintains both YAML (`configs/experiments/`) and legacy JSON (`results/configs/`) with fallback logic, creating maintenance burden. The JSON path is dead code.

3. **Pipeline orchestration**: `run_pipeline()` has 7 hardcoded steps with nested try/except blocks, duplicate data loading (limiter runs twice), and creates 3+ intermediate files instead of using in-memory structures.

4. **Massive config overrides**: Functions build 30+ line nested dicts to override configs, indicating the config system isn't fit for purpose. This pattern repeats in `run_experiment()`, `train_model()`, and `rank_symbols()`.

5. **Progress bars reinvented**: Custom progress bar implementations in `apply_trend_prefilter()` and `rank_symbols()` that should use `tqdm` or similar.

### Dead Code & Synthetic Paths

1. **Unused JSON config loading**: `rank_symbols()` loads `config_path = f"results/configs/{exp_name}.json"` but never uses it - immediately loads YAML instead.

2. **Hidden fallback dependencies**: `setup_virtual_environment()` has hardcoded package list `core_deps` that installs synthetic versions if `pip install -e .` fails - this masks real installation issues.

3. **Zero-filled missing features**: In `rank_symbols()`, missing features are silently filled with `0.0`: `last_row[c] = 0.0`. This is **hidden fake data** that produces invalid predictions without warning.

4. **Unused parameter**: `rank_symbols()` accepts `downside_k` but computes `stock_downside_k` internally, ignoring the input.

5. **Temporary file anti-pattern**: `run_pipeline()` creates `temp_limiter_json` and `temp_config_path` files just to delete them - underlying functions should accept objects.

### Unnecessary Configuration Flags

1. **Verbose/quiet flags**: Multiple functions have both `verbose` and `quiet` parameters with overlapping behavior. The `quiet` flag in `rank_symbols()` still writes files but suppresses console output, creating inconsistent state.

2. **Threshold overrides**: `rank_symbols()` accepts `threshold` but falls back to config, then global default, creating three layers of indirection.

3. **Model type detection**: Stringly-typed model type checks (`if str(model_type_label).lower() == 'equal_weight'`) instead of proper enums or classes.

## Refactor Plan

### Phase 1: Delete Dead Code (Safe Deletions)
- Remove all venv management functions (`check_virtual_environment`, `get_venv_python`, `setup_virtual_environment`, `restart_with_venv`) and the venv restart block at line 140
- Delete JSON config support from `list_configs()` and `run_experiment()`
- Remove `downside_k` parameter from `rank_symbols()` signature
- Delete `config_path` variable in `rank_symbols()` (line 1045)
- Remove synthetic `core_deps` fallback in `setup_virtual_environment()` (lines 67-75)

### Phase 2: Fix Data Integrity (Critical Changes)
- **Remove zero-filling**: Replace `last_row[c] = 0.0` with `raise ValueError(f"Missing feature {c} for symbol {symbol}")`
- **Make fallbacks explicit**: Replace all `getattr(config, 'param', default)` with required config fields using `pydantic` models
- **Validate early**: Add `validate_config_complete()` that checks all required features exist before ranking
- **Remove synthetic trend data**: In `apply_trend_prefilter()`, change `(no_data)` keep logic to explicit filter mode parameter

### Phase 3: Simplify Architecture (Reorganization)
1. **Consolidate pipeline**: Replace `run_pipeline()` with a `Pipeline` class that accepts step objects and passes DataFrames between them in memory
2. **Unified config**: Create `Config` pydantic model that loads from YAML only; remove all config override dicts
3. **Single progress bar**: Replace custom progress logic with `tqdm` and remove `VGM_PLAIN_PROGRESS` hack
4. **Path management**: Add `Paths` class with constants for all directories; replace string paths like `'results/reports'`

### Phase 4: Improve Error Handling
- Replace `except Exception as e: print(...); return None` with specific exceptions that propagate up
- Remove `traceback.print_exc()` calls; configure logging with `logging.exception()`
- Make `run_pipeline()` fail fast instead of continuing with partial state

### Migration Checklist

```bash
# 1. Backup current version
cp vgmini.py vgmini.py.backup

# 2. Remove venv logic (delete lines 23-142)
# 3. Install pydantic if not present
pip install pydantic

# 4. Create config_models.py with pydantic schemas
# 5. Replace config override dicts with typed config objects
# 6. Add tqdm to requirements
# 7. Run validation script to catch missing features
python -c "from src.config_validator import validate; validate()"

# 8. Test ranking with incomplete data (should now fail fast)
python vgmini.py rank test_model --verbose

# 9. Update all experiment YAMLs to include confidence_threshold
# 10. Remove results/configs/ directory after migration confirmed
```

**Expected outcome**: ~40% reduction in LOC, elimination of hidden synthetic data paths, and fail-fast behavior that ensures data integrity.

=== LLM RUN ===
time=2025-11-29 20:31 | model=perplexity/sonar | preset=refactor-ai-mess | kind=function | codebase=vgmini | temp=0.10 | max_tok=512 | in=4789 | out=474 | dur=8.763s | cost=0.005737
command=./vg list

PROMPT:
You are an expert Python engineer. Review the following function or execution slice, which was likely generated or heavily edited by AI. Provide a concise analysis that covers:
- Where the structure is overcomplicated, redundant, or over-abstracted
- Any dead code, unused branches, or unnecessary configuration flags
- Any use of synthetic or fallback data paths that should be removed or made explicit (for technical analysis software, there must be no hidden fake data).

Then propose a refactor plan:
- How to simplify while preserving behavior and data integrity
- Which pieces can safely be deleted vs. just reorganized
- A short checklist of steps to get from the current version to a cleaner one

Function source or trace context:
```python
"""
Experiment configuration templates for different trading strategies and scenarios
"""
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Union
import json

@dataclass
class TechnicalIndicatorConfig:
    """Configuration for technical indicators"""
    rsi: Dict[str, Union[int, float]] = field(default_factory=lambda: {
        "period": 14,
        "oversold_threshold": 30,
        "overbought_threshold": 70
    })
    macd: Dict[str, int] = field(default_factory=lambda: {
        "fast_period": 12,
        "slow_period": 26,
        "signal_period": 9
    })
    bollinger_bands: Dict[str, Union[int, float]] = field(default_factory=lambda: {
        "period": 20,
        "std_dev": 2
    })
    momentum: Dict[str, Union[int, float]] = field(default_factory=lambda: {
        "window": 5,
        "std_window": 20,
        "std_multiplier": 1.5
    })
    roc_periods: List[int] = field(default_factory=lambda: [5, 10])
    volume_roc_periods: List[int] = field(default_factory=lambda: [5])

@dataclass
class AnalysisConfig:
    """Configuration for analysis parameters"""
    interpretability_sample_size: int = 1000
    lime_sample_size: int = 100
    lime_max_samples: int = 100
    min_valid_target_ratio: float = 0.8
    max_correlation_threshold: float = 0.95
    rolling_sharpe_window: int = 60
    trading_days_per_year: int = 252

@dataclass
class VisualizationConfig:
    """Configuration for visualization parameters"""
    dpi: int = 300
    figure_size: List[int] = field(default_factory=lambda: [12, 8])

@dataclass
class ExperimentConfig:
    """Configuration for a trading experiment"""
    name: str
    description: str
    
    # Data selection
    symbols: List[str]
    start_date: str
    end_date: str
    
    # Model parameters
    buy_threshold: float = 0.02
    sell_threshold: float = 0.02
    forecast_horizon: int = 5
    buy_forecast_horizon: Optional[int] = None
    sell_forecast_horizon: Optional[int] = None
    confidence_threshold: float = 0.65
    
    # Target strategy
    target_strategy: str = 'dual'  # 'dual' or 'buy_only'
    
    # Feature engineering
    ema_periods: List[int] = None
    volume_window: int = 20
    
    # Model training
    test_size: float = 0.2
    class_weight: str = 'balanced'
    penalty: str = 'l1'
    solver: str = 'liblinear'
    max_iter: int = 1000
    tol: float = 1e-4
    model_type: str = 'logistic'
    
    # Ensemble specific
    base_models: Optional[List[str]] = None
    weights: Optional[List[float]] = None
    
    # Equal-weight specific parameters (optional)
    ew_correlation_signs: bool = False
    ew_calibrate_prior: bool = False
    ew_activation_threshold: float = 0.0
    ew_temperature: float = 1.0
    ew_buy_aggregation: str = 'fraction'
    ew_sell_aggregation: str = 'fraction'
    
    # XGBoost specific parameters
    n_estimators: int = 100
    max_depth: int = 6
    learning_rate: float = 0.1
    subsample: float = 0.8
    colsample_bytree: float = 0.8
    
    # Backtesting
    initial_capital: float = 100000.0
    commission: float = 0.001
    slippage: float = 0.0005
    max_position_size: float = 0.1
    # Thresholding
    threshold_strategy: str = 'absolute'  # 'absolute' or 'percentile'
    threshold_percentile: float = 0.7
    threshold_window: int = 60
    # Trade frequency/gating
    auto_threshold_fallback: bool = True
    fallback_percentile: float = 0.7
    fallback_window: int = 60
    buy_consecutive_days: int = 1
    sell_consecutive_days: int = 1
    hysteresis_margin: float = 0.0
    trade_cooldown_days: int = 0
    min_holding_days: int = 0
    
    # New configurable parameters
    technical_indicators: TechnicalIndicatorConfig = field(default_factory=TechnicalIndicatorConfig)
    enabled_features: List[str] = field(default_factory=lambda: [
        "price_above_ema9", "price_above_ema30", "price_momentum_strong",
        "ema9_above_ema30", "ema_spread_accelerating",
        "macd_above_signal", "macd_above_zero", "signal_above_zero", "macd_positive_hist",
        "volume_above_average", "volume_ratio", "ha_uptrend",
        "rsi_oversold", "rsi_overbought", "rsi_bullish",
        "price_roc_5", "price_roc_10", "volume_roc_5",
        "ema_separation_accel", "price_ema_separation_accel"
    ])
    analysis: AnalysisConfig = field(default_factory=AnalysisConfig)
    visualization: VisualizationConfig = field(default_factory=VisualizationConfig)
    
    def __post_init__(self):
        if self.ema_periods is None:
            self.ema_periods = [9, 21, 50]
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'name': self.name,
            'description': self.description,
            'symbols': self.symbols,
            'start_date': self.start_date,
            'end_date': self.end_date,
            'buy_threshold': self.buy_threshold,
            'sell_threshold': self.sell_threshold,
            'forecast_horizon': self.forecast_horizon,
            'buy_forecast_horizon': self.buy_forecast_horizon,
            'sell_forecast_horizon': self.sell_forecast_horizon,
            'confidence_threshold': self.confidence_threshold,
            'ema_periods': self.ema_periods,
            'volume_window': self.volume_window,
            'test_size': self.test_size,
            'class_weight': self.class_weight,
            'penalty': self.penalty,
            'solver': self.solver,
            'max_iter': self.max_iter,
            'tol': self.tol,
            'model_type': self.model_type,
             'target_strategy': getattr(self, 'target_strategy', 'dual'),
             'base_models': getattr(self, 'base_models', None),
             'weights': getattr(self, 'weights', None),
            # Equal-weight params
            'ew_correlation_signs': self.ew_correlation_signs,
            'ew_calibrate_prior': self.ew_calibrate_prior,
            'ew_activation_threshold': self.ew_activation_threshold,
            'ew_temperature': self.ew_temperature,
            'ew_buy_aggregation': self.ew_buy_aggregation,
            'ew_sell_aggregation': self.ew_sell_aggregation,
            'n_estimators': self.n_estimators,
            'max_depth': self.max_depth,
            'learning_rate': self.learning_rate,
            'subsample': self.subsample,
            'colsample_bytree': self.colsample_bytree,
            'initial_capital': self.initial_capital,
            'commission': self.commission,
             'slippage': getattr(self, 'slippage', 0.0005),
            'max_position_size': self.max_position_size,
             'threshold_strategy': self.threshold_strategy,
             'threshold_percentile': self.threshold_percentile,
             'threshold_window': self.threshold_window,
             'auto_threshold_fallback': getattr(self, 'auto_threshold_fallback', True),
             'fallback_percentile': getattr(self, 'fallback_percentile', 0.7),
             'fallback_window': getattr(self, 'fallback_window', 60),
             'buy_consecutive_days': getattr(self, 'buy_consecutive_days', 1),
             'sell_consecutive_days': getattr(self, 'sell_consecutive_days', 1),
             'hysteresis_margin': getattr(self, 'hysteresis_margin', 0.0),
             'trade_cooldown_days': getattr(self, 'trade_cooldown_days', 0),
             'min_holding_days': getattr(self, 'min_holding_days', 0),
             'threshold_strategy': self.threshold_strategy,
             'threshold_percentile': self.threshold_percentile,
             'threshold_window': self.threshold_window,
            'technical_indicators': {
                'rsi': self.technical_indicators.rsi,
                'macd': self.technical_indicators.macd,
                'bollinger_bands': self.technical_indicators.bollinger_bands,
                'momentum': self.technical_indicators.momentum,
                'roc_periods': self.technical_indicators.roc_periods,
                'volume_roc_periods': self.technical_indicators.volume_roc_periods
            },
            'enabled_features': self.enabled_features,
            'analysis': {
                'interpretability_sample_size': self.analysis.interpretability_sample_size,
                'lime_sample_size': self.analysis.lime_sample_size,
                'lime_max_samples': self.analysis.lime_max_samples,
                'min_valid_target_ratio': self.analysis.min_valid_target_ratio,
                'max_correlation_threshold': self.analysis.max_correlation_threshold,
                'rolling_sharpe_window': self.analysis.rolling_sharpe_window,
                'trading_days_per_year': self.analysis.trading_days_per_year
            },
            'visualization': {
                'dpi': self.visualization.dpi,
                'figure_size': self.visualization.figure_size
            }
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ExperimentConfig':
        """Create from dictionary"""
        # Handle nested configuration objects
        data_copy = data.copy()
        
        # Parse technical_indicators if present
        if 'technical_indicators' in data_copy:
            tech_data = data_copy.pop('technical_indicators')
            data_copy['technical_indicators'] = TechnicalIndicatorConfig(
                rsi=tech_data.get('rsi', TechnicalIndicatorConfig().rsi),
                macd=tech_data.get('macd', TechnicalIndicatorConfig().macd),
                bollinger_bands=tech_data.get('bollinger_bands', TechnicalIndicatorConfig().bollinger_bands),
                momentum=tech_data.get('momentum', TechnicalIndicatorConfig().momentum),
                roc_periods=tech_data.get('roc_periods', TechnicalIndicatorConfig().roc_periods),
                volume_roc_periods=tech_data.get('volume_roc_periods', TechnicalIndicatorConfig().volume_roc_periods)
            )
        
        # Parse analysis if present
        if 'analysis' in data_copy:
            analysis_data = data_copy.pop('analysis')
            data_copy['analysis'] = AnalysisConfig(
                interpretability_sample_size=analysis_data.get('interpretability_sample_size', 1000),
                lime_sample_size=analysis_data.get('lime_sample_size', 100),
                lime_max_samples=analysis_data.get('lime_max_samples', 100),
                min_valid_target_ratio=analysis_data.get('min_valid_target_ratio', 0.8),
                max_correlation_threshold=analysis_data.get('max_correlation_threshold', 0.95),
                rolling_sharpe_window=analysis_data.get('rolling_sharpe_window', 60),
                trading_days_per_year=analysis_data.get('trading_days_per_year', 252)
            )
        
        # Parse visualization if present
        if 'visualization' in data_copy:
            viz_data = data_copy.pop('visualization')
            data_copy['visualization'] = VisualizationConfig(
                dpi=viz_data.get('dpi', 300),
                figure_size=viz_data.get('figure_size', [12, 8])
            )
        
        # Optional top-level additions for Sprint 2
        data_copy['target_strategy'] = data_copy.get('target_strategy', data_copy.get('target', 'dual'))
        data_copy['base_models'] = data_copy.get('base_models')
        data_copy['weights'] = data_copy.get('weights')
        return cls(**data_copy)  # includes slippage if present
    
    def save_to_file(self, filepath: str):
        """Save configuration to JSON file"""
        with open(filepath, 'w') as f:
            json.dump(self.to_dict(), f, indent=2)
    
    @classmethod
    def load_from_file(cls, filepath: str) -> 'ExperimentConfig':
        """Load configuration from JSON file"""
        with open(filepath, 'r') as f:
            data = json.load(f)
        return cls.from_dict(data)

# Predefined experiment configurations
EXPERIMENT_CONFIGS = {
    'quick_test': ExperimentConfig(
        name='quick_test',
        description='Quick test with major ETFs for development and debugging',
        symbols=['SPY', 'QQQ', 'IWM'],
        start_date='2023-01-01',
        end_date='2024-01-01',
        buy_threshold=0.02,
        sell_threshold=0.02,
        forecast_horizon=5,
        confidence_threshold=0.6,
        ema_periods=[9, 21, 50],
        initial_capital=50000.0
    ),
    
    'conservative_strategy': ExperimentConfig(
        name='conservative_strategy',
        description='Conservative strategy with lower thresholds and longer horizon',
        symbols=['SPY', 'QQQ', 'DIA', 'IWM', 'VTI'],
        start_date='2022-01-01',
        end_date='2024-01-01',
        buy_threshold=0.015,  # 1.5% threshold
        sell_threshold=0.015,
        forecast_horizon=10,   # Longer horizon
        confidence_threshold=0.7,  # Higher confidence
        ema_periods=[9, 21, 50, 100],
        commission=0.0005,     # Lower commission
        max_position_size=0.05  # Smaller positions
    ),
    
    'aggressive_strategy': ExperimentConfig(
        name='aggressive_strategy',
        description='Aggressive strategy with higher thresholds and shorter horizon',
        symbols=['QQQ', 'TQQQ', 'ARKK', 'SPXL'],
        start_date='2023-01-01',
        end_date='2024-01-01',
        buy_threshold=0.03,    # 3% threshold
        sell_threshold=0.03,
        forecast_horizon=3,    # Shorter horizon
        confidence_threshold=0.55,  # Lower confidence for more trades
        ema_periods=[5, 13, 21],    # Faster EMAs
        commission=0.002,      # Higher commission for frequent trading
        max_position_size=0.15  # Larger positions
    ),
    
    'tech_stocks': ExperimentConfig(
        name='tech_stocks',
        description='Technology stocks analysis',
        symbols=['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META'],
        start_date='2022-01-01',
        end_date='2024-01-01',
        buy_threshold=0.025,
        sell_threshold=0.025,
        forecast_horizon=7,
        confidence_threshold=0.65,
        ema_periods=[9, 21, 50],
        initial_capital=200000.0
    ),
    
    'dividend_stocks': ExperimentConfig(
        name='dividend_stocks',
        description='Dividend-focused stocks with conservative approach',
        symbols=['JNJ', 'PG', 'KO', 'PFE', 'VZ', 'T', 'XOM'],
        start_date='2022-01-01',
        end_date='2024-01-01',
        buy_threshold=0.01,    # Lower threshold for stable stocks
        sell_threshold=0.015,
        forecast_horizon=15,   # Longer horizon for stable stocks
        confidence_threshold=0.75,  # Higher confidence
        ema_periods=[21, 50, 100, 200],  # Longer EMAs
        max_position_size=0.08
    ),
    
    'sector_rotation': ExperimentConfig(
        name='sector_rotation',
        description='Sector ETFs for rotation strategy',
        symbols=['XLK', 'XLF', 'XLE', 'XLV', 'XLI', 'XLY', 'XLP', 'XLU', 'XLB'],
        start_date='2021-01-01',
        end_date='2024-01-01',
        buy_threshold=0.02,
        sell_threshold=0.02,
        forecast_horizon=10,
        confidence_threshold=0.6,
        ema_periods=[9, 21, 50],
        initial_capital=300000.0,
        max_position_size=0.12  # Larger positions for sector rotation
    ),
    
    'small_cap_growth': ExperimentConfig(
        name='small_cap_growth',
        description='Small cap growth stocks with higher volatility parameters',
        symbols=['IWM', 'VB', 'VTWO', 'SCHA'],
        start_date='2023-01-01',
        end_date='2024-01-01',
        buy_threshold=0.04,    # Higher threshold for volatile stocks
        sell_threshold=0.04,
        forecast_horizon=5,
        confidence_threshold=0.6,
        ema_periods=[5, 13, 21, 50],
        commission=0.0015,     # Higher commission for small caps
        max_position_size=0.08
    ),
    
    'crypto_related': ExperimentConfig(
        name='crypto_related',
        description='Cryptocurrency-related stocks and ETFs',
        symbols=['COIN', 'MSTR', 'RIOT', 'MARA', 'BITO'],
        start_date='2023-01-01',
        end_date='2024-01-01',
        buy_threshold=0.05,    # Very high threshold for crypto volatility
        sell_threshold=0.05,
        forecast_horizon=3,    # Short horizon for high volatility
        confidence_threshold=0.5,  # Lower confidence for more signals
        ema_periods=[5, 10, 21],   # Very fast EMAs
        commission=0.002,
        max_position_size=0.05  # Smaller positions due to high risk
    ),
    
    'dual_signal': ExperimentConfig(
        name='dual_signal',
        description='Dual logistic regression model as described in convo.txt - separate buy/sell models with crossover signals',
        symbols=['SPY', 'QQQ'],
        start_date='2023-01-01',
        end_date='2024-01-01',
        buy_threshold=0.02,    # 2% gain threshold for buy signal success
        sell_threshold=0.02,   # 2% loss threshold for sell signal success
        forecast_horizon=5,    # 5-day forecast horizon (reduced for smaller datasets)
        confidence_threshold=0.65,  # 65% confidence threshold as discussed in convo.txt
        ema_periods=[9, 21, 50],    # EMA periods from codex.txt (adapted for data size)
        volume_window=20,
        test_size=0.2,
        class_weight='balanced',
        penalty='l1',          # L1 regularization for feature selection
        solver='liblinear',    # Solver that supports L1 penalty
        initial_capital=100000.0,
        commission=0.001,      # 0.1% commission
        max_position_size=0.1  # Max 10% per position
    )
}

def list_available_configs() -> Dict[str, str]:
    """List all available experiment configurations"""
    return {name: config.description for name, config in EXPERIMENT_CONFIGS.items()}

def get_config(name: str) -> ExperimentConfig:
    """Get a specific experiment configuration"""
    if name not in EXPERIMENT_CONFIGS:
        available = list(EXPERIMENT_CONFIGS.keys())
        raise ValueError(f"Configuration '{name}' not found. Available: {available}")
    return EXPERIMENT_CONFIGS[name]

def create_custom_config(
    name: str,
    symbols: List[str],
    start_date: str,
    end_date: str,
    **kwargs
) -> ExperimentConfig:
    """Create a custom experiment configuration"""
    return ExperimentConfig(
        name=name,
        description=f"Custom configuration: {name}",
        symbols=symbols,
        start_date=start_date,
        end_date=end_date,
        **kwargs
    )

def save_config_templates():
    """Save all predefined configurations to files"""
    import os
    os.makedirs('results/configs', exist_ok=True)
    
    for name, config in EXPERIMENT_CONFIGS.items():
        filepath = f'results/configs/{name}.json'
        config.save_to_file(filepath)
        print(f"Saved {name} configuration to {filepath}")

if __name__ == "__main__":
    # Save all configuration templates
    save_config_templates()
    
    # List available configurations
    print("\nAvailable Experiment Configurations:")
    print("=" * 50)
    for name, description in list_available_configs().items():
        print(f"{name:20} - {description}")
```

RESPONSE:
# Analysis of ExperimentConfig Structure

## Critical Issues

**Redundancy in `to_dict()` method:**
The method contains **duplicate keys** that will silently overwrite each other[1]. Lines serialize `threshold_strategy`, `threshold_percentile`, and `threshold_window` twiceâ€”the second assignment overwrites the first. This is a data integrity bug, not just poor style.

```python
'threshold_strategy': self.threshold_strategy,           # Line 1
'threshold_percentile': self.threshold_percentile,       # Line 1
'threshold_window': self.threshold_window,               # Line 1
# ... later ...
'threshold_strategy': self.threshold_strategy,           # Line 2 (overwrites)
'threshold_percentile': self.threshold_percentile,       # Line 2 (overwrites)
'threshold_window': self.threshold_window,               # Line 2 (overwrites)
```

**Inconsistent defensive coding in `to_dict()`:**
Some fields use `getattr()` with fallbacks (e.g., `slippage`, `auto_threshold_fallback`) while others access attributes directly. This is contradictoryâ€”if a field has a default value in the dataclass definition, `getattr()` is unnecessary. If it doesn't, all fields should use `getattr()` for safety[1].

**Over-parameterized dataclass:**
The `ExperimentConfig` has **50+ fields**, many of which are strategy-specific (e.g., `ew_correlation_signs`, `ew_calibrate_prior` for equal-weight; XGBoost hyperparameters). This violates the principle that dataclasses should be lightweight, explicit structures[1]. The class conflates multiple concerns: data selection, model configuration, backtesting parameters, and algorithm-specific tuning.

**Synthetic/fallback data in `from_dict()`:**
The method contains a hidden fallback path:
```python
data_copy['target_strategy'] = data_copy.get('target_strategy', data_copy.get('target', 'dual'))
```
This silently accepts a legacy `'target'` key and maps it to `'target_strategy'`. This

=== LLM RUN ===
time=2025-11-30 13:39 | model=perplexity/sonar | preset=bugs-edges | kind=function | codebase=vgmini | temp=0.10 | max_tok=512 | in=4616 | out=512 | dur=9.326s | cost=0.005640
command=./vg pipeline --model equal_weight_dual --no-render

PROMPT:
This code runs a pipeline of several functions: decorrelate, limiter etc. Can you locate where these functions are executed, i want to be able to re-order the functions as needed


Function source or trace context:
```python
from __future__ import annotations

import os
import json
import shutil
from datetime import datetime, timedelta
from typing import List, Dict, Any


def extract_buy_signals(ranking_file: str) -> str:
    """Extract only BUY signals from a ranking file.

    This is the core implementation moved from src.cli_pipeline, kept
    behavior-identical. The CLI layer should just call this.
    """
    buy_signals_file = ranking_file.replace(".txt", "_buy_signals.txt")

    try:
        with open(ranking_file, "r") as f:
            lines = f.readlines()

        print(f"DEBUG: Ranking file has {len(lines)} total lines")

        buy_lines: List[str] = []
        buy_count = 0

        # Debug: Show first few lines to understand format
        print("DEBUG: First 10 lines of ranking file:")
        for i, line in enumerate(lines[:10]):
            print(f"  Line {i+1}: {repr(line.strip())}")

        # Process each line
        for line_num, line in enumerate(lines):
            if line.startswith("#") or line.startswith("Rank") or line.startswith("-"):
                # Keep header lines
                buy_lines.append(line)
                continue

            # Parse ranking line to check signal
            parts = line.strip().split()

            # Should have Rank, Symbol, Score1, Score2, Emoji, Signal columns
            if len(parts) >= 6:
                signal = parts[5]  # Signal column (after emoji)
                if signal == "BUY":
                    buy_lines.append(line)
                    buy_count += 1

        print(f"âœ“ Extracted {buy_count} BUY signals from {line_num+1} lines")

        # Write BUY signals file
        with open(buy_signals_file, "w") as f:
            f.writelines(buy_lines)

        # Verify written file
        with open(buy_signals_file, "r") as f:
            written_lines = f.readlines()
        print(f"DEBUG: Written file has {len(written_lines)} lines")

        print(f"Extracted {buy_count} BUY signals from {ranking_file}")
        return buy_signals_file

    except Exception as e:
        print(f"ERROR: Could not extract BUY signals: {e}")
        import traceback

        traceback.print_exc()
        return ranking_file


def count_lines(file_path: str) -> int:
    """Count non-header lines in a file."""
    try:
        with open(file_path, "r") as f:
            lines = f.readlines()
        return len(
            [
                line
                for line in lines
                if not line.startswith("#")
                and not line.startswith("Rank")
                and not line.startswith("-")
                and line.strip()
            ]
        )
    except Exception:
        return 0


def apply_trend_prefilter(ranking_file: str, limiter_json_file: str) -> str:
    """Pre-filter ranking file to remove bearish/consolidating stocks before decorrelation."""
    import pandas as pd  # noqa: F401
    from datetime import datetime  # noqa: F401
    import traceback

    print("ğŸ” Applying trend-based pre-filtering...")

    # Load limiter analysis results
    if not os.path.exists(limiter_json_file):
        print(f"ERROR: Limiter file not found: {limiter_json_file}")
        return ranking_file

    try:
        with open(limiter_json_file, "r") as f:
            limiter_data = json.load(f)

        print(f"DEBUG: Loaded limiter data with keys: {list(limiter_data.keys())}")

        # Create lookup of trend classifications
        trend_lookup: Dict[str, Any] = {}
        if "results" in limiter_data:
            print(f"DEBUG: Limiter has {len(limiter_data['results'])} results")
            for i, result in enumerate(limiter_data["results"]):
                symbol = result.get("symbol")
                trend = result.get("trend")
                trend_strength = result.get("trend_strength", 0)
                trend_lookup[symbol] = (trend, trend_strength)
                if i < 5:  # Show first 5 for debugging
                    print(
                        f"DEBUG: Trend lookup - {symbol}: {trend} ({trend_strength})"
                    )
        else:
            print("DEBUG: No 'results' key found in limiter data")

        print(f"DEBUG: Created trend lookup for {len(trend_lookup)} symbols")

        # Read original ranking file
        filtered_lines: List[str] = []
        removed_stocks: List[str] = []
        kept_stocks: List[str] = []

        with open(ranking_file, "r") as f:
            lines = f.readlines()

        print(f"ğŸ“Š Processing {len(lines)} ranking lines for trend filtering...")

        # Check for duplicate lines in input
        unique_lines = list(dict.fromkeys(lines))  # Remove duplicates, preserve order
        if len(unique_lines) != len(lines):
            print(f"âš ï¸  Removed {len(lines) - len(unique_lines)} duplicate lines")
            lines = unique_lines

        # Process each line with progress tracking
        processed_symbols = set()  # Track processed symbols to avoid double-processing
        total_lines = len(lines)
        for line_num, line in enumerate(lines):
            # Hypermodern gradient progress bar every 50 lines
            if line_num % 50 == 0 or line_num == total_lines - 1:
                progress = int(30 * (line_num + 1) / total_lines)
                bar = "â–ˆ" * progress + "â–‘" * (30 - progress)
                print(
                    f"\r[{bar}] {line_num+1}/{total_lines} lines processed",
                    end="",
                    flush=True,
                )
            if (
                line.startswith("#")
                or line.startswith("Rank")
                or line.startswith("-")
            ):
                # Keep header lines
                filtered_lines.append(line)
                continue

            # Parse ranking line
            parts = line.strip().split()
            if len(parts) >= 2:
                symbol = parts[1]

                # Skip if we've already processed this symbol (prevent duplicates)
                if symbol in processed_symbols:
                    continue
                processed_symbols.add(symbol)

                # Check trend classification
                if symbol in trend_lookup:
                    trend, trend_strength = trend_lookup[symbol]

                    # Filter criteria: Only keep bullish and strong breakout trends
                    if trend == "bullish" or (
                        trend == "breakout" and trend_strength > 0.7
                    ):
                        filtered_lines.append(line)
                        kept_stocks.append(f"{symbol}({trend})")
                    else:
                        removed_stocks.append(f"{symbol}({trend})")
                else:
                    # Keep if no trend data available
                    filtered_lines.append(line)
                    kept_stocks.append(f"{symbol}(no_data)")
            else:
                filtered_lines.append(line)

        # Complete progress bar
        print()  # New line after progress bar

        # Write filtered ranking file
        output_file = ranking_file.replace(".txt", "_trend_filtered.txt")
        with open(output_file, "w") as f:
            f.writelines(filtered_lines)

        # Clean summary without excessive debug
        breakout_count = sum(1 for stock in kept_stocks if "(breakout)" in stock)
        bullish_count = sum(1 for stock in kept_stocks if "(bullish)" in stock)
        bearish_removed = sum(
            1 for stock in removed_stocks if "(bearish)" in stock
        )
        consolidating_removed = sum(
            1 for stock in removed_stocks if "(consolidating)" in stock
        )

        print(
            f"âœ“ Trend filtering complete: kept {len(kept_stocks)} stocks "
            f"({breakout_count} breakouts, {bullish_count} bullish)"
        )
        print(
            f"  Removed {len(removed_stocks)} stocks "
            f"({bearish_removed} bearish, {consolidating_removed} consolidating)"
        )

        return output_file

    except Exception as e:
        print(f"ERROR: Could not apply trend pre-filter: {e}")
        traceback.print_exc()
        return ranking_file


def run_pipeline(model_name: str, top_count: int = 100, no_render: bool = False):
    """Run the complete VGMini pipeline: ranking -> decorrelation -> trailstop -> limiter -> charts.

    This is the core implementation (formerly in src.cli_pipeline.run_pipeline).
    """
    from .ranking import rank_symbols as core_rank_symbols
    from .limiter import run_limiter_with_data
    from .data_loader import StockDataLoader
    from .decorrelate import run_decorrelate, load_config
    from .trailstop import run_trailstop
    from .render_charts import render_charts_with_trailstop
    import logging
    import tempfile
    import yaml

    # Step 1: Generate ranking using the core ranking helper
    print(f"[pipeline] Ranking top {top_count} symbols...")
    
    # Check if ranking file exists and is current (optimization)
    ranking_file_path = f"results/{model_name}_ranking.txt"
    if os.path.exists(ranking_file_path):
        try:
            # Check if file contains latest market data (not today's calendar date)
            from datetime import datetime
            from .data_loader import StockDataLoader
            
            # Get latest available market data date from database
            loader = StockDataLoader()
            summary = loader.get_data_summary(interval="1d")
            latest_market_date = summary.get("date_range", {}).get("end_date", None)
            loader.close()
            
            if latest_market_date:
                # Parse the latest market date (handle both string and timestamp formats)
                import pandas as pd
                latest_date_str = pd.to_datetime(latest_market_date).strftime('%Y-%m-%d')
                
                # Check if ranking file contains the latest market data date
                with open(ranking_file_path, 'r') as f:
                    content = f.read()
                    if latest_date_str in content:
                        print(f"ğŸ“… Ranking file is current ({latest_date_str}) - skipping expensive rank operation")
                        ranking_file = ranking_file_path
                    else:
                        print(f"ğŸ“… Ranking file exists but outdated - regenerating for latest market data ({latest_date_str})")
                        ranking_file = None
            else:
                # Fallback: regenerate if we can't determine latest market date
                print("ğŸ“… Cannot determine latest market date - regenerating ranking")
                ranking_file = None
        except Exception as e:
            # If we can't read/parse the file or check dates, regenerate it
            print(f"ğŸ“… Error checking ranking file currency: {e} - regenerating")
            ranking_file = None
    else:
        ranking_file = None
    
    # Generate ranking if needed
    if ranking_file is None:
        try:
            ranking_file = core_rank_symbols(
                model_name,
                top_n=top_count,
                verbose=False,
                threshold=None,
                sort_by="er",
                quiet=True,
            )
        except Exception as e:
            print(f"[pipeline] Ranking failed: {e}")
            return

    # Verify ranking file was created
    if not ranking_file or not os.path.exists(ranking_file):
        # Fallback to legacy path (for older experiments)
        legacy_ranking_file = f"results/{model_name}_ranking.txt"
        if os.path.exists(legacy_ranking_file):
            ranking_file = legacy_ranking_file
        else:
            print(f"[pipeline] Error: Ranking file not found: {ranking_file}")
            return
    # Ranking file written (reduce console clutter)

    # Step 2: Extract BUY signals and analyze with limiter
    print("[pipeline] Extracting BUY signals for trend analysis...")
    try:
        # Extract only BUY signals from ranking file
        buy_signals_file = extract_buy_signals(ranking_file)
        print(f"Extracted {count_lines(buy_signals_file)} BUY signals for analysis")

        # Run limiter analysis on BUY signals only
        logging.getLogger("src.data_loader").setLevel(logging.WARNING)

        # Load data for symbols in BUY signals
        symbols_data: Dict[str, Any] = {}
        loader = StockDataLoader()

        # Read symbols from BUY signals file
        buy_symbols: List[str] = []
        with open(buy_signals_file, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2 and parts[0].isdigit():
                    buy_symbols.append(parts[1])

        # Load data for each symbol (no quality validation needed - already verified)
        for symbol in buy_symbols:
            try:
                df = loader.get_symbol_data(
                    symbol,
                    start_date=(
                        datetime.now() - timedelta(days=180)
                    ).strftime("%Y-%m-%d"),
                )
                if not df.empty:
                    symbols_data[symbol] = df
            except Exception as e:
                print(f"   Warning: Could not load data for {symbol}: {e}")

        loader.close()

        if symbols_data:
            temp_limiter_result = run_limiter_with_data(
                symbols_data, out_dir="results/reports"
            )
            # Save the JSON result to a file for compatibility with existing code
            temp_limiter_json_path = "results/reports/limiter.json"
            os.makedirs("results/reports", exist_ok=True)
            with open(temp_limiter_json_path, "w") as f:
                json.dump(temp_limiter_result, f, indent=2)
            temp_limiter_json = temp_limiter_json_path
        else:
            raise Exception("No data loaded for BUY signals analysis")

        # Apply pre-filtering to remove bearish/consolidating stocks (single pass only)
        filtered_ranking_file = apply_trend_prefilter(
            buy_signals_file, temp_limiter_json
        )

    except Exception as e:
        print(f"[pipeline] BUY signal filtering failed: {e}")
        print("Continuing with original ranking...")
        filtered_ranking_file = ranking_file

    # Step 3: Decorrelate from pre-filtered signals
    print("[pipeline] Decorrelating from trend-filtered signals...")
    try:
        # Load the decorrelate config and update the model name
        config = load_config("configs/decorrelate.yaml")
        # Don't override ranking_file in config - we'll pass filtered_ranking_file as override

        # Replace model name placeholder in output prefix
        if "output" in config and "prefix" in config["output"]:
            config["output"]["prefix"] = config["output"]["prefix"].replace(
                "{{model_name}}", model_name
            )

        # Write temporary config file
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".yaml", delete=False
        ) as temp_config:
            yaml.dump(config, temp_config)
            temp_config_path = temp_config.name

        try:
            out_txt, out_json = run_decorrelate(
                temp_config_path, filtered_ranking_file
            )
            print(f"De-correlated shortlist: {out_txt}")

            # NOTE: Commented out root ranking file generation - mithra reads from results/decorrelated/
            # If mithra.sh breaks, uncomment these lines:
            # vgmini_ranking_file = f"{model_name}_ranking.txt"  # For reports.py to find
            # if os.path.exists(out_txt):
            #     shutil.copy2(out_txt, vgmini_ranking_file)  # Copy to vgmini root for reports.py
            #     print(f"Final ranking saved to: {vgmini_ranking_file}")
            #     ranking_file = vgmini_ranking_file  # Update for next steps
            
            # Use decorrelated file directly - single source of truth
            ranking_file = out_txt
            print(f"Final ranking: {ranking_file}")

        finally:
            # Clean up temporary config file
            if os.path.exists(temp_config_path):
                os.unlink(temp_config_path)

    except Exception as e:
        print(f"[pipeline] Decorrelation failed: {e}")
        return

    # Step 3: Trail stops
    print("[pipeline] Computing trail stops...")
    try:
        trailstop_json = run_trailstop(out_txt, "results/reports")
        # Trailstop analysis completed
        # Ensure we use the JSON file for chart rendering
        if trailstop_json.endswith(".csv"):
            trailstop_json = "results/reports/trailstop.json"
    except Exception as e:
        print(f"[pipeline] Trail stop analysis failed: {e}")
        return

    # Step 4: Limiter analysis
    print("[pipeline] Analyzing swing highs/lows...")
    try:
        logging.getLogger("src.data_loader").setLevel(logging.WARNING)

        # Load data for symbols in final ranking
        symbols_data = {}
        loader = StockDataLoader()

        # Read symbols from final ranking file
        final_symbols: List[str] = []
        with open(out_txt, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2 and parts[0].isdigit():
                    final_symbols.append(parts[1])

        # Load data for each symbol (no quality validation needed - already verified)
        for symbol in final_symbols:
            try:
                df = loader.get_symbol_data(
                    symbol,
                    start_date=(
                        datetime.now() - timedelta(days=180)
                    ).strftime("%Y-%m-%d"),
                )
                if not df.empty:
                    symbols_data[symbol] = df
            except Exception as e:
                print(f"   Warning: Could not load data for {symbol}: {e}")

        loader.close()

        if symbols_data:
            limiter_result = run_limiter_with_data(
                symbols_data, out_dir="results/reports"
            )
            # Save the JSON result to a file for compatibility with existing code
            limiter_json = "results/reports/limiter.json"
        else:
            raise Exception("No data loaded for limiter analysis")
        # Limiter analysis completed
    except Exception as e:
        print(f"[pipeline] Limiter analysis failed: {e}")
        return

    # Step 5: Render charts (optional)
    if no_render:
        print("[pipeline] Skipping chart rendering (--no-render flag)")
    else:
        print("[pipeline] Rendering charts...")
        try:
            # Ensure reports directory exists
            os.makedirs("results/reports", exist_ok=True)
            # Parse symbols from the final ranking file (decorrelated)
            symbols: List[str] = []
            if os.path.exists(ranking_file):
                with open(ranking_file, "r") as f:
                    for line in f:
                        line = line.strip()
                        if (
                            line
                            and not line.startswith("#")
                            and not line.startswith("De-correlated")
                            and not line.startswith("Total")
                        ):
                            parts = line.split()
                            if (
                                len(parts) >= 2
                                and parts[0].isdigit()
                            ):  # Must start with rank number
                                symbols.append(
                                    parts[1]
                                )  # Symbol is typically second column

            if symbols:
                # Pass both trailstop and limiter data for comprehensive chart overlays
                limiter_json = "results/reports/limiter.json"
                render_charts_with_trailstop(
                    symbols,
                    "results/reports",
                    trailstop_json,
                    ranking_file,
                    limiter_json,
                )
                # Charts rendered successfully
                if len(symbols) <= 10:  # Only show symbols if 10 or fewer
                    print(f"   Symbols: {', '.join(symbols)}")
                # Top symbols already shown in confidence ordering
            else:
                print(f"[pipeline] No symbols found in ranking file: {ranking_file}")
        except Exception as e:
            print(f"[pipeline] Chart rendering failed: {e}")
            return

    # Step 6: Create model config reference for mithra.sh
    print("[pipeline] Preparing model configuration reference...")
    try:
        # Create a simple reference file pointing to the YAML config
        config_ref_file = f"results/{model_name}_config.yaml"
        yaml_config_path = f"configs/experiments/{model_name}.yaml"

        if os.path.exists(yaml_config_path):
            # Create symlink or copy to results for easier access by mithra.sh
            shutil.copy2(yaml_config_path, config_ref_file)
            # Config reference created
            print(f"Source config: {yaml_config_path}")
        else:
            print(f"Warning: Source config not found: {yaml_config_path}")

    except Exception as e:
        print(f"Warning: Config reference creation failed: {e}")

    # Step 7: Summary
    print("[pipeline] Pipeline completed successfully!")
    print("ğŸ“Š Final outputs for mithra.sh:")
    print(f"   ğŸ“‹ Ranking: {ranking_file} (decorrelated)")
    print("   ğŸ“ˆ Charts: results/reports/")
    print(
        f"   âš™ï¸  Model config: configs/experiments/{model_name}.yaml (editable)"
    )
    print("   ğŸ›¡ï¸  Trailstop: results/reports/trailstop.json")
    print("   âš–ï¸  Limiter: results/reports/limiter.json")
    print("\nâœ… Ready for mithra.sh integration!")
```

RESPONSE:
The functions in your pipeline (`extract_buy_signals`, `apply_trend_prefilter`, `run_decorrelate`, `run_trailstop`, `run_limiter_with_data`, `render_charts_with_trailstop`, etc.) are executed **sequentially** within the `run_pipeline` function, and the **order of execution is explicitly defined by the code structure**.

### Where are the functions executed?

The execution happens in the `run_pipeline` function, where each step is called one after another:

```python
# Step 1: Generate ranking
ranking_file = core_rank_symbols(...)

# Step 2: Extract BUY signals and analyze with limiter
buy_signals_file = extract_buy_signals(ranking_file)
filtered_ranking_file = apply_trend_prefilter(buy_signals_file, temp_limiter_json)

# Step 3: Decorrelate from pre-filtered signals
out_txt, out_json = run_decorrelate(temp_config_path, filtered_ranking_file)

# Step 3: Trail stops
trailstop_json = run_trailstop(out_txt, "results/reports")

# Step 4: Limiter analysis
limiter_result = run_limiter_with_data(symbols_data, out_dir="results/reports")

# Step 5: Render charts (optional)
render_charts_with_trailstop(symbols, "results/reports", trailstop_json, ranking_file, limiter_json)

# Step 6: Create model config reference
shutil.copy2(yaml_config_path, config_ref_file)
```

### How to Re-Order the Functions

To **re-order the pipeline steps**, you simply need to **re-arrange the sequence of these function calls** in the `run_pipeline` function.

For example, if you want to run `run_trailstop` before `run_decorrelate`, you would move the `run_trailstop` call before the `run_decorrelate` call:

```python
# Step 3: Trail stops
trailstop_json = run_trailstop(ranking_file, "results/reports")

# Step 4: Decorrelate from pre-filtered signals
out_txt, out_json = run_decorrelate(temp_config_path, filtered_ranking_file)
```

### Important Notes

- **Dependencies Matter**: Some functions depend on the output of previous steps. For example, `run_decorrelate` needs the output of `apply_trend_prefilter`, and `render_charts_with_trailstop` needs the output

